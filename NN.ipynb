{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author: Tianyou Xiao(txiao3) & Ziyu Song(zsong10)\n",
    "# CSC240 Final Project\n",
    "# Dec. 12th, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 # learning rate set to 0.01\n",
    "BATCH_SIZE = 64 # batch size set  to 64\n",
    "EPOCH = 250 # run for 250 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "#75:25 train:test\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    # load ratings into 2d numpy array\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network's structure\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 50)\n",
    "        self.fc4 = nn.Linear(50, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.16952997\n",
      "Epoch:  1 | Step:  0 | train loss:  0.14799552\n",
      "Epoch:  2 | Step:  0 | train loss:  0.12715906\n",
      "Epoch:  3 | Step:  0 | train loss:  0.10669372\n",
      "Epoch:  4 | Step:  0 | train loss:  0.08686035\n",
      "Epoch:  5 | Step:  0 | train loss:  0.068255045\n",
      "Epoch:  6 | Step:  0 | train loss:  0.05148868\n",
      "Epoch:  7 | Step:  0 | train loss:  0.03726017\n",
      "Epoch:  8 | Step:  0 | train loss:  0.026145343\n",
      "Epoch:  9 | Step:  0 | train loss:  0.01843946\n",
      "Epoch:  10 | Step:  0 | train loss:  0.0141458465\n",
      "Epoch:  11 | Step:  0 | train loss:  0.012979386\n",
      "Epoch:  12 | Step:  0 | train loss:  0.01425484\n",
      "Epoch:  13 | Step:  0 | train loss:  0.016896825\n",
      "Epoch:  14 | Step:  0 | train loss:  0.019705849\n",
      "Epoch:  15 | Step:  0 | train loss:  0.021741267\n",
      "Epoch:  16 | Step:  0 | train loss:  0.022530701\n",
      "Epoch:  17 | Step:  0 | train loss:  0.022037413\n",
      "Epoch:  18 | Step:  0 | train loss:  0.02052057\n",
      "Epoch:  19 | Step:  0 | train loss:  0.018387424\n",
      "Epoch:  20 | Step:  0 | train loss:  0.016073074\n",
      "Epoch:  21 | Step:  0 | train loss:  0.013955019\n",
      "Epoch:  22 | Step:  0 | train loss:  0.012301626\n",
      "Epoch:  23 | Step:  0 | train loss:  0.011250341\n",
      "Epoch:  24 | Step:  0 | train loss:  0.010810612\n",
      "Epoch:  25 | Step:  0 | train loss:  0.010886763\n",
      "Epoch:  26 | Step:  0 | train loss:  0.011314096\n",
      "Epoch:  27 | Step:  0 | train loss:  0.01190046\n",
      "Epoch:  28 | Step:  0 | train loss:  0.012465165\n",
      "Epoch:  29 | Step:  0 | train loss:  0.012868263\n",
      "Epoch:  30 | Step:  0 | train loss:  0.013026408\n",
      "Epoch:  31 | Step:  0 | train loss:  0.012915546\n",
      "Epoch:  32 | Step:  0 | train loss:  0.012563616\n",
      "Epoch:  33 | Step:  0 | train loss:  0.012037243\n",
      "Epoch:  34 | Step:  0 | train loss:  0.011425622\n",
      "Epoch:  35 | Step:  0 | train loss:  0.010823686\n",
      "Epoch:  36 | Step:  0 | train loss:  0.010316032\n",
      "Epoch:  37 | Step:  0 | train loss:  0.009963143\n",
      "Epoch:  38 | Step:  0 | train loss:  0.009791829\n",
      "Epoch:  39 | Step:  0 | train loss:  0.009791929\n",
      "Epoch:  40 | Step:  0 | train loss:  0.009920649\n",
      "Epoch:  41 | Step:  0 | train loss:  0.010114207\n",
      "Epoch:  42 | Step:  0 | train loss:  0.010304141\n",
      "Epoch:  43 | Step:  0 | train loss:  0.01043385\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010470651\n",
      "Epoch:  45 | Step:  0 | train loss:  0.010410303\n",
      "Epoch:  46 | Step:  0 | train loss:  0.010273585\n",
      "Epoch:  47 | Step:  0 | train loss:  0.010097036\n",
      "Epoch:  48 | Step:  0 | train loss:  0.009921265\n",
      "Epoch:  49 | Step:  0 | train loss:  0.009780278\n",
      "Epoch:  50 | Step:  0 | train loss:  0.009694338\n",
      "Epoch:  51 | Step:  0 | train loss:  0.009667457\n",
      "Epoch:  52 | Step:  0 | train loss:  0.009689285\n",
      "Epoch:  53 | Step:  0 | train loss:  0.009740118\n",
      "Epoch:  54 | Step:  0 | train loss:  0.009797245\n",
      "Epoch:  55 | Step:  0 | train loss:  0.009840875\n",
      "Epoch:  56 | Step:  0 | train loss:  0.00985834\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009845905\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009808189\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009755716\n",
      "Epoch:  60 | Step:  0 | train loss:  0.00970141\n",
      "Epoch:  61 | Step:  0 | train loss:  0.00965698\n",
      "Epoch:  62 | Step:  0 | train loss:  0.009630057\n",
      "Epoch:  63 | Step:  0 | train loss:  0.0096227145\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009631624\n",
      "Epoch:  65 | Step:  0 | train loss:  0.0096496735\n",
      "Epoch:  66 | Step:  0 | train loss:  0.009668457\n",
      "Epoch:  67 | Step:  0 | train loss:  0.009680841\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009682828\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009674247\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009658203\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009639616\n",
      "Epoch:  72 | Step:  0 | train loss:  0.0096234465\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009613183\n",
      "Epoch:  74 | Step:  0 | train loss:  0.009610029\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009612907\n",
      "Epoch:  76 | Step:  0 | train loss:  0.009619161\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009625621\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009629655\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009629882\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009626397\n",
      "Epoch:  81 | Step:  0 | train loss:  0.009620498\n",
      "Epoch:  82 | Step:  0 | train loss:  0.009614083\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009608947\n",
      "Epoch:  84 | Step:  0 | train loss:  0.0096062105\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009606055\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009607801\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009610264\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009612236\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009612899\n",
      "Epoch:  90 | Step:  0 | train loss:  0.009612049\n",
      "Epoch:  91 | Step:  0 | train loss:  0.0096100615\n",
      "Epoch:  92 | Step:  0 | train loss:  0.00960767\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009605641\n",
      "Epoch:  94 | Step:  0 | train loss:  0.009604494\n",
      "Epoch:  95 | Step:  0 | train loss:  0.009604359\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009604985\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009605901\n",
      "Epoch:  98 | Step:  0 | train loss:  0.0096066175\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009606812\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009606417\n",
      "Epoch:  101 | Step:  0 | train loss:  0.009605603\n",
      "Epoch:  102 | Step:  0 | train loss:  0.009604678\n",
      "Epoch:  103 | Step:  0 | train loss:  0.009603943\n",
      "Epoch:  104 | Step:  0 | train loss:  0.009603582\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009603613\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009603897\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009604223\n",
      "Epoch:  108 | Step:  0 | train loss:  0.009604399\n",
      "Epoch:  109 | Step:  0 | train loss:  0.009604327\n",
      "Epoch:  110 | Step:  0 | train loss:  0.009604029\n",
      "Epoch:  111 | Step:  0 | train loss:  0.009603618\n",
      "Epoch:  112 | Step:  0 | train loss:  0.009603241\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009603015\n",
      "Epoch:  114 | Step:  0 | train loss:  0.009602967\n",
      "Epoch:  115 | Step:  0 | train loss:  0.009603045\n",
      "Epoch:  116 | Step:  0 | train loss:  0.009603158\n",
      "Epoch:  117 | Step:  0 | train loss:  0.009603207\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009603131\n",
      "Epoch:  119 | Step:  0 | train loss:  0.0096029155\n",
      "Epoch:  120 | Step:  0 | train loss:  0.0096026\n",
      "Epoch:  121 | Step:  0 | train loss:  0.009602256\n",
      "Epoch:  122 | Step:  0 | train loss:  0.0096019525\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009601705\n",
      "Epoch:  124 | Step:  0 | train loss:  0.009601466\n",
      "Epoch:  125 | Step:  0 | train loss:  0.009601194\n",
      "Epoch:  126 | Step:  0 | train loss:  0.009600884\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009600565\n",
      "Epoch:  128 | Step:  0 | train loss:  0.009600293\n",
      "Epoch:  129 | Step:  0 | train loss:  0.009600093\n",
      "Epoch:  130 | Step:  0 | train loss:  0.009599947\n",
      "Epoch:  131 | Step:  0 | train loss:  0.009599845\n",
      "Epoch:  132 | Step:  0 | train loss:  0.009599775\n",
      "Epoch:  133 | Step:  0 | train loss:  0.009599718\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009599648\n",
      "Epoch:  135 | Step:  0 | train loss:  0.009599537\n",
      "Epoch:  136 | Step:  0 | train loss:  0.009599363\n",
      "Epoch:  137 | Step:  0 | train loss:  0.009599149\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009598956\n",
      "Epoch:  139 | Step:  0 | train loss:  0.009598811\n",
      "Epoch:  140 | Step:  0 | train loss:  0.0095987\n",
      "Epoch:  141 | Step:  0 | train loss:  0.009598608\n",
      "Epoch:  142 | Step:  0 | train loss:  0.009598519\n",
      "Epoch:  143 | Step:  0 | train loss:  0.009598425\n",
      "Epoch:  144 | Step:  0 | train loss:  0.0095983185\n",
      "Epoch:  145 | Step:  0 | train loss:  0.0095982\n",
      "Epoch:  146 | Step:  0 | train loss:  0.009598076\n",
      "Epoch:  147 | Step:  0 | train loss:  0.009597952\n",
      "Epoch:  148 | Step:  0 | train loss:  0.009597829\n",
      "Epoch:  149 | Step:  0 | train loss:  0.0095977085\n",
      "Epoch:  150 | Step:  0 | train loss:  0.009597588\n",
      "Epoch:  151 | Step:  0 | train loss:  0.009597464\n",
      "Epoch:  152 | Step:  0 | train loss:  0.009597334\n",
      "Epoch:  153 | Step:  0 | train loss:  0.009597198\n",
      "Epoch:  154 | Step:  0 | train loss:  0.0095970575\n",
      "Epoch:  155 | Step:  0 | train loss:  0.009596914\n",
      "Epoch:  156 | Step:  0 | train loss:  0.009596772\n",
      "Epoch:  157 | Step:  0 | train loss:  0.009596629\n",
      "Epoch:  158 | Step:  0 | train loss:  0.009596485\n",
      "Epoch:  159 | Step:  0 | train loss:  0.009596337\n",
      "Epoch:  160 | Step:  0 | train loss:  0.009596185\n",
      "Epoch:  161 | Step:  0 | train loss:  0.009596029\n",
      "Epoch:  162 | Step:  0 | train loss:  0.009595869\n",
      "Epoch:  163 | Step:  0 | train loss:  0.009595707\n",
      "Epoch:  164 | Step:  0 | train loss:  0.009595544\n",
      "Epoch:  165 | Step:  0 | train loss:  0.00959538\n",
      "Epoch:  166 | Step:  0 | train loss:  0.009595213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.009595044\n",
      "Epoch:  168 | Step:  0 | train loss:  0.009594871\n",
      "Epoch:  169 | Step:  0 | train loss:  0.009594696\n",
      "Epoch:  170 | Step:  0 | train loss:  0.009594518\n",
      "Epoch:  171 | Step:  0 | train loss:  0.009594338\n",
      "Epoch:  172 | Step:  0 | train loss:  0.009594157\n",
      "Epoch:  173 | Step:  0 | train loss:  0.009593975\n",
      "Epoch:  174 | Step:  0 | train loss:  0.00959379\n",
      "Epoch:  175 | Step:  0 | train loss:  0.009593604\n",
      "Epoch:  176 | Step:  0 | train loss:  0.009593415\n",
      "Epoch:  177 | Step:  0 | train loss:  0.009593225\n",
      "Epoch:  178 | Step:  0 | train loss:  0.009593034\n",
      "Epoch:  179 | Step:  0 | train loss:  0.009592841\n",
      "Epoch:  180 | Step:  0 | train loss:  0.009592649\n",
      "Epoch:  181 | Step:  0 | train loss:  0.009592455\n",
      "Epoch:  182 | Step:  0 | train loss:  0.009592259\n",
      "Epoch:  183 | Step:  0 | train loss:  0.009592064\n",
      "Epoch:  184 | Step:  0 | train loss:  0.009591867\n",
      "Epoch:  185 | Step:  0 | train loss:  0.009591671\n",
      "Epoch:  186 | Step:  0 | train loss:  0.009591473\n",
      "Epoch:  187 | Step:  0 | train loss:  0.009591276\n",
      "Epoch:  188 | Step:  0 | train loss:  0.009591079\n",
      "Epoch:  189 | Step:  0 | train loss:  0.009590883\n",
      "Epoch:  190 | Step:  0 | train loss:  0.009590686\n",
      "Epoch:  191 | Step:  0 | train loss:  0.00959049\n",
      "Epoch:  192 | Step:  0 | train loss:  0.009590295\n",
      "Epoch:  193 | Step:  0 | train loss:  0.0095901005\n",
      "Epoch:  194 | Step:  0 | train loss:  0.009589907\n",
      "Epoch:  195 | Step:  0 | train loss:  0.009589714\n",
      "Epoch:  196 | Step:  0 | train loss:  0.009589522\n",
      "Epoch:  197 | Step:  0 | train loss:  0.009589332\n",
      "Epoch:  198 | Step:  0 | train loss:  0.009589144\n",
      "Epoch:  199 | Step:  0 | train loss:  0.009588957\n",
      "Epoch:  200 | Step:  0 | train loss:  0.009588771\n",
      "Epoch:  201 | Step:  0 | train loss:  0.009588587\n",
      "Epoch:  202 | Step:  0 | train loss:  0.0095884055\n",
      "Epoch:  203 | Step:  0 | train loss:  0.009588225\n",
      "Epoch:  204 | Step:  0 | train loss:  0.009588047\n",
      "Epoch:  205 | Step:  0 | train loss:  0.009587871\n",
      "Epoch:  206 | Step:  0 | train loss:  0.009587698\n",
      "Epoch:  207 | Step:  0 | train loss:  0.009587525\n",
      "Epoch:  208 | Step:  0 | train loss:  0.009587357\n",
      "Epoch:  209 | Step:  0 | train loss:  0.00958719\n",
      "Epoch:  210 | Step:  0 | train loss:  0.009587025\n",
      "Epoch:  211 | Step:  0 | train loss:  0.009586863\n",
      "Epoch:  212 | Step:  0 | train loss:  0.009586704\n",
      "Epoch:  213 | Step:  0 | train loss:  0.0095865475\n",
      "Epoch:  214 | Step:  0 | train loss:  0.009586393\n",
      "Epoch:  215 | Step:  0 | train loss:  0.009586241\n",
      "Epoch:  216 | Step:  0 | train loss:  0.009586092\n",
      "Epoch:  217 | Step:  0 | train loss:  0.009585945\n",
      "Epoch:  218 | Step:  0 | train loss:  0.0095858015\n",
      "Epoch:  219 | Step:  0 | train loss:  0.00958566\n",
      "Epoch:  220 | Step:  0 | train loss:  0.009585521\n",
      "Epoch:  221 | Step:  0 | train loss:  0.009585385\n",
      "Epoch:  222 | Step:  0 | train loss:  0.009585252\n",
      "Epoch:  223 | Step:  0 | train loss:  0.009585121\n",
      "Epoch:  224 | Step:  0 | train loss:  0.009584992\n",
      "Epoch:  225 | Step:  0 | train loss:  0.009584866\n",
      "Epoch:  226 | Step:  0 | train loss:  0.009584743\n",
      "Epoch:  227 | Step:  0 | train loss:  0.009584622\n",
      "Epoch:  228 | Step:  0 | train loss:  0.009584503\n",
      "Epoch:  229 | Step:  0 | train loss:  0.009584388\n",
      "Epoch:  230 | Step:  0 | train loss:  0.009584273\n",
      "Epoch:  231 | Step:  0 | train loss:  0.009584162\n",
      "Epoch:  232 | Step:  0 | train loss:  0.0095840525\n",
      "Epoch:  233 | Step:  0 | train loss:  0.009583945\n",
      "Epoch:  234 | Step:  0 | train loss:  0.009583841\n",
      "Epoch:  235 | Step:  0 | train loss:  0.009583739\n",
      "Epoch:  236 | Step:  0 | train loss:  0.009583637\n",
      "Epoch:  237 | Step:  0 | train loss:  0.009583538\n",
      "Epoch:  238 | Step:  0 | train loss:  0.0095834425\n",
      "Epoch:  239 | Step:  0 | train loss:  0.0095833475\n",
      "Epoch:  240 | Step:  0 | train loss:  0.009583254\n",
      "Epoch:  241 | Step:  0 | train loss:  0.009583163\n",
      "Epoch:  242 | Step:  0 | train loss:  0.009583074\n",
      "Epoch:  243 | Step:  0 | train loss:  0.009582986\n",
      "Epoch:  244 | Step:  0 | train loss:  0.0095828995\n",
      "Epoch:  245 | Step:  0 | train loss:  0.009582815\n",
      "Epoch:  246 | Step:  0 | train loss:  0.009582732\n",
      "Epoch:  247 | Step:  0 | train loss:  0.009582651\n",
      "Epoch:  248 | Step:  0 | train loss:  0.00958257\n",
      "Epoch:  249 | Step:  0 | train loss:  0.009582492\n",
      "Runtime: 417.22192335128784seconds\n"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "loss_his = []\n",
    "st = time.time()\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader): \n",
    "        # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())\n",
    "\n",
    "print('Runtime: ' + str(time.time()-st) + 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training loss visualization\n",
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.9682, 3.4754, 3.1795,  ..., 3.5001, 3.5000, 4.0001],\n",
       "         [3.9682, 3.4753, 3.1795,  ..., 3.5000, 3.5000, 4.0000],\n",
       "         [3.9665, 3.4739, 3.1785,  ..., 3.4983, 3.4985, 3.9982],\n",
       "         ...,\n",
       "         [3.9682, 3.4754, 3.1795,  ..., 3.5001, 3.5000, 4.0001],\n",
       "         [3.9682, 3.4753, 3.1795,  ..., 3.5000, 3.5000, 4.0000],\n",
       "         [3.9682, 3.4754, 3.1795,  ..., 3.5001, 3.5000, 4.0001]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.1848960143229332\n",
      "Autoencoder MAE: 1.403978564758373\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(get_mae(out, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
