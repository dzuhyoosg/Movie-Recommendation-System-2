{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.1702938\n",
      "Epoch:  1 | Step:  0 | train loss:  0.16078098\n",
      "Epoch:  2 | Step:  0 | train loss:  0.15140396\n",
      "Epoch:  3 | Step:  0 | train loss:  0.14191265\n",
      "Epoch:  4 | Step:  0 | train loss:  0.13232112\n",
      "Epoch:  5 | Step:  0 | train loss:  0.122674026\n",
      "Epoch:  6 | Step:  0 | train loss:  0.1129463\n",
      "Epoch:  7 | Step:  0 | train loss:  0.103103176\n",
      "Epoch:  8 | Step:  0 | train loss:  0.09319996\n",
      "Epoch:  9 | Step:  0 | train loss:  0.08336309\n",
      "Epoch:  10 | Step:  0 | train loss:  0.07363341\n",
      "Epoch:  11 | Step:  0 | train loss:  0.06421376\n",
      "Epoch:  12 | Step:  0 | train loss:  0.055226002\n",
      "Epoch:  13 | Step:  0 | train loss:  0.046831142\n",
      "Epoch:  14 | Step:  0 | train loss:  0.03919196\n",
      "Epoch:  15 | Step:  0 | train loss:  0.0324472\n",
      "Epoch:  16 | Step:  0 | train loss:  0.02670423\n",
      "Epoch:  17 | Step:  0 | train loss:  0.022012586\n",
      "Epoch:  18 | Step:  0 | train loss:  0.018388309\n",
      "Epoch:  19 | Step:  0 | train loss:  0.015785119\n",
      "Epoch:  20 | Step:  0 | train loss:  0.014099518\n",
      "Epoch:  21 | Step:  0 | train loss:  0.013189542\n",
      "Epoch:  22 | Step:  0 | train loss:  0.012885787\n",
      "Epoch:  23 | Step:  0 | train loss:  0.013009297\n",
      "Epoch:  24 | Step:  0 | train loss:  0.013393466\n",
      "Epoch:  25 | Step:  0 | train loss:  0.0138913095\n",
      "Epoch:  26 | Step:  0 | train loss:  0.014386275\n",
      "Epoch:  27 | Step:  0 | train loss:  0.014798768\n",
      "Epoch:  28 | Step:  0 | train loss:  0.015074316\n",
      "Epoch:  29 | Step:  0 | train loss:  0.015183042\n",
      "Epoch:  30 | Step:  0 | train loss:  0.015120132\n",
      "Epoch:  31 | Step:  0 | train loss:  0.01489671\n",
      "Epoch:  32 | Step:  0 | train loss:  0.014538085\n",
      "Epoch:  33 | Step:  0 | train loss:  0.01407835\n",
      "Epoch:  34 | Step:  0 | train loss:  0.013554243\n",
      "Epoch:  35 | Step:  0 | train loss:  0.0130013395\n",
      "Epoch:  36 | Step:  0 | train loss:  0.012454803\n",
      "Epoch:  37 | Step:  0 | train loss:  0.011943988\n",
      "Epoch:  38 | Step:  0 | train loss:  0.011490352\n",
      "Epoch:  39 | Step:  0 | train loss:  0.011108675\n",
      "Epoch:  40 | Step:  0 | train loss:  0.010806274\n",
      "Epoch:  41 | Step:  0 | train loss:  0.010583582\n",
      "Epoch:  42 | Step:  0 | train loss:  0.010434913\n",
      "Epoch:  43 | Step:  0 | train loss:  0.010349826\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010314939\n",
      "Epoch:  45 | Step:  0 | train loss:  0.010315025\n",
      "Epoch:  46 | Step:  0 | train loss:  0.010334903\n",
      "Epoch:  47 | Step:  0 | train loss:  0.010360669\n",
      "Epoch:  48 | Step:  0 | train loss:  0.010380956\n",
      "Epoch:  49 | Step:  0 | train loss:  0.010387395\n",
      "Epoch:  50 | Step:  0 | train loss:  0.010374991\n",
      "Epoch:  51 | Step:  0 | train loss:  0.010342208\n",
      "Epoch:  52 | Step:  0 | train loss:  0.010290492\n",
      "Epoch:  53 | Step:  0 | train loss:  0.010223593\n",
      "Epoch:  54 | Step:  0 | train loss:  0.010146831\n",
      "Epoch:  55 | Step:  0 | train loss:  0.010066258\n",
      "Epoch:  56 | Step:  0 | train loss:  0.009987784\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009916442\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009855959\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009808508\n",
      "Epoch:  60 | Step:  0 | train loss:  0.009774671\n",
      "Epoch:  61 | Step:  0 | train loss:  0.009753585\n",
      "Epoch:  62 | Step:  0 | train loss:  0.00974327\n",
      "Epoch:  63 | Step:  0 | train loss:  0.0097410055\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009743796\n",
      "Epoch:  65 | Step:  0 | train loss:  0.009748628\n",
      "Epoch:  66 | Step:  0 | train loss:  0.009752947\n",
      "Epoch:  67 | Step:  0 | train loss:  0.009754897\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009753404\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009748143\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009739409\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009727963\n",
      "Epoch:  72 | Step:  0 | train loss:  0.009714863\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009701277\n",
      "Epoch:  74 | Step:  0 | train loss:  0.009688307\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009676855\n",
      "Epoch:  76 | Step:  0 | train loss:  0.009667516\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009660561\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009655969\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009653414\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009652393\n",
      "Epoch:  81 | Step:  0 | train loss:  0.009652333\n",
      "Epoch:  82 | Step:  0 | train loss:  0.009652677\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009652934\n",
      "Epoch:  84 | Step:  0 | train loss:  0.009652731\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009651827\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009650162\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009647983\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009645398\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009642614\n",
      "Epoch:  90 | Step:  0 | train loss:  0.009639861\n",
      "Epoch:  91 | Step:  0 | train loss:  0.009637333\n",
      "Epoch:  92 | Step:  0 | train loss:  0.0096351635\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009633403\n",
      "Epoch:  94 | Step:  0 | train loss:  0.00963207\n",
      "Epoch:  95 | Step:  0 | train loss:  0.009631144\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009630496\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009629972\n",
      "Epoch:  98 | Step:  0 | train loss:  0.00962937\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009628755\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009627827\n",
      "Epoch:  101 | Step:  0 | train loss:  0.009626817\n",
      "Epoch:  102 | Step:  0 | train loss:  0.00962573\n",
      "Epoch:  103 | Step:  0 | train loss:  0.009624546\n",
      "Epoch:  104 | Step:  0 | train loss:  0.0096233385\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009622104\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009620862\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009619586\n",
      "Epoch:  108 | Step:  0 | train loss:  0.009618374\n",
      "Epoch:  109 | Step:  0 | train loss:  0.009617233\n",
      "Epoch:  110 | Step:  0 | train loss:  0.009616235\n",
      "Epoch:  111 | Step:  0 | train loss:  0.009615365\n",
      "Epoch:  112 | Step:  0 | train loss:  0.00961456\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009613793\n",
      "Epoch:  114 | Step:  0 | train loss:  0.009613044\n",
      "Epoch:  115 | Step:  0 | train loss:  0.009612277\n",
      "Epoch:  116 | Step:  0 | train loss:  0.009611446\n",
      "Epoch:  117 | Step:  0 | train loss:  0.009610436\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009609525\n",
      "Epoch:  119 | Step:  0 | train loss:  0.0096087\n",
      "Epoch:  120 | Step:  0 | train loss:  0.009607855\n",
      "Epoch:  121 | Step:  0 | train loss:  0.009606984\n",
      "Epoch:  122 | Step:  0 | train loss:  0.009606096\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009605205\n",
      "Epoch:  124 | Step:  0 | train loss:  0.009604335\n",
      "Epoch:  125 | Step:  0 | train loss:  0.009603494\n",
      "Epoch:  126 | Step:  0 | train loss:  0.009602652\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009601196\n",
      "Epoch:  128 | Step:  0 | train loss:  0.009600445\n",
      "Epoch:  129 | Step:  0 | train loss:  0.0095996335\n",
      "Epoch:  130 | Step:  0 | train loss:  0.009598751\n",
      "Epoch:  131 | Step:  0 | train loss:  0.009597858\n",
      "Epoch:  132 | Step:  0 | train loss:  0.009596989\n",
      "Epoch:  133 | Step:  0 | train loss:  0.009596063\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009595121\n",
      "Epoch:  135 | Step:  0 | train loss:  0.009594281\n",
      "Epoch:  136 | Step:  0 | train loss:  0.009593499\n",
      "Epoch:  137 | Step:  0 | train loss:  0.0095927855\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009592129\n",
      "Epoch:  139 | Step:  0 | train loss:  0.009591501\n",
      "Epoch:  140 | Step:  0 | train loss:  0.009590868\n",
      "Epoch:  141 | Step:  0 | train loss:  0.0095901955\n",
      "Epoch:  142 | Step:  0 | train loss:  0.009589426\n",
      "Epoch:  143 | Step:  0 | train loss:  0.009588412\n",
      "Epoch:  144 | Step:  0 | train loss:  0.009587299\n",
      "Epoch:  145 | Step:  0 | train loss:  0.009586287\n",
      "Epoch:  146 | Step:  0 | train loss:  0.0095850965\n",
      "Epoch:  147 | Step:  0 | train loss:  0.009583848\n",
      "Epoch:  148 | Step:  0 | train loss:  0.0095826145\n",
      "Epoch:  149 | Step:  0 | train loss:  0.009581369\n",
      "Epoch:  150 | Step:  0 | train loss:  0.009580145\n",
      "Epoch:  151 | Step:  0 | train loss:  0.009578856\n",
      "Epoch:  152 | Step:  0 | train loss:  0.00957741\n",
      "Epoch:  153 | Step:  0 | train loss:  0.009575789\n",
      "Epoch:  154 | Step:  0 | train loss:  0.009574245\n",
      "Epoch:  155 | Step:  0 | train loss:  0.009572768\n",
      "Epoch:  156 | Step:  0 | train loss:  0.009571299\n",
      "Epoch:  157 | Step:  0 | train loss:  0.009569759\n",
      "Epoch:  158 | Step:  0 | train loss:  0.009568058\n",
      "Epoch:  159 | Step:  0 | train loss:  0.0095661385\n",
      "Epoch:  160 | Step:  0 | train loss:  0.009563973\n",
      "Epoch:  161 | Step:  0 | train loss:  0.00956153\n",
      "Epoch:  162 | Step:  0 | train loss:  0.009558857\n",
      "Epoch:  163 | Step:  0 | train loss:  0.009556065\n",
      "Epoch:  164 | Step:  0 | train loss:  0.009552884\n",
      "Epoch:  165 | Step:  0 | train loss:  0.009548754\n",
      "Epoch:  166 | Step:  0 | train loss:  0.009544673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.009541221\n",
      "Epoch:  168 | Step:  0 | train loss:  0.0095382165\n",
      "Epoch:  169 | Step:  0 | train loss:  0.009535241\n",
      "Epoch:  170 | Step:  0 | train loss:  0.009531614\n",
      "Epoch:  171 | Step:  0 | train loss:  0.009522538\n",
      "Epoch:  172 | Step:  0 | train loss:  0.009518559\n",
      "Epoch:  173 | Step:  0 | train loss:  0.009514661\n",
      "Epoch:  174 | Step:  0 | train loss:  0.0095104445\n",
      "Epoch:  175 | Step:  0 | train loss:  0.009505554\n",
      "Epoch:  176 | Step:  0 | train loss:  0.009494686\n",
      "Epoch:  177 | Step:  0 | train loss:  0.009471618\n",
      "Epoch:  178 | Step:  0 | train loss:  0.009452474\n",
      "Epoch:  179 | Step:  0 | train loss:  0.009443435\n",
      "Epoch:  180 | Step:  0 | train loss:  0.009434538\n",
      "Epoch:  181 | Step:  0 | train loss:  0.0094234655\n",
      "Epoch:  182 | Step:  0 | train loss:  0.009409699\n",
      "Epoch:  183 | Step:  0 | train loss:  0.0093795955\n",
      "Epoch:  184 | Step:  0 | train loss:  0.009341056\n",
      "Epoch:  185 | Step:  0 | train loss:  0.009311398\n",
      "Epoch:  186 | Step:  0 | train loss:  0.009280057\n",
      "Epoch:  187 | Step:  0 | train loss:  0.009244153\n",
      "Epoch:  188 | Step:  0 | train loss:  0.009216589\n",
      "Epoch:  189 | Step:  0 | train loss:  0.009192408\n",
      "Epoch:  190 | Step:  0 | train loss:  0.009164234\n",
      "Epoch:  191 | Step:  0 | train loss:  0.009105039\n",
      "Epoch:  192 | Step:  0 | train loss:  0.009080176\n",
      "Epoch:  193 | Step:  0 | train loss:  0.009059347\n",
      "Epoch:  194 | Step:  0 | train loss:  0.009029296\n",
      "Epoch:  195 | Step:  0 | train loss:  0.009005957\n",
      "Epoch:  196 | Step:  0 | train loss:  0.008981615\n",
      "Epoch:  197 | Step:  0 | train loss:  0.008961867\n",
      "Epoch:  198 | Step:  0 | train loss:  0.008939235\n",
      "Epoch:  199 | Step:  0 | train loss:  0.008922163\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader):          # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8582,  3.3811,  3.2836,  ..., -0.1012,  3.3142, -1.1228],\n",
       "         [ 3.5561,  3.1029,  3.0706,  ..., -0.1060,  3.0687, -1.0393],\n",
       "         [ 3.8874,  3.4117,  3.3106,  ..., -0.0993,  3.3367, -1.1222],\n",
       "         ...,\n",
       "         [ 4.0618,  3.5632,  3.4291,  ..., -0.1029,  3.4755, -1.1684],\n",
       "         [ 4.0079,  3.5156,  3.3934,  ..., -0.1009,  3.4321, -1.1539],\n",
       "         [ 4.0046,  3.5123,  3.3857,  ..., -0.1035,  3.4327, -1.1591]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.2107372522220277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(math.sqrt(get_mse(out, test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
