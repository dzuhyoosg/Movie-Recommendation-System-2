{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.17306961\n",
      "Epoch:  1 | Step:  0 | train loss:  0.16402377\n",
      "Epoch:  2 | Step:  0 | train loss:  0.15536454\n",
      "Epoch:  3 | Step:  0 | train loss:  0.14661609\n",
      "Epoch:  4 | Step:  0 | train loss:  0.137816\n",
      "Epoch:  5 | Step:  0 | train loss:  0.12903991\n",
      "Epoch:  6 | Step:  0 | train loss:  0.120333605\n",
      "Epoch:  7 | Step:  0 | train loss:  0.11179808\n",
      "Epoch:  8 | Step:  0 | train loss:  0.103457615\n",
      "Epoch:  9 | Step:  0 | train loss:  0.09534051\n",
      "Epoch:  10 | Step:  0 | train loss:  0.087500244\n",
      "Epoch:  11 | Step:  0 | train loss:  0.079835854\n",
      "Epoch:  12 | Step:  0 | train loss:  0.07234945\n",
      "Epoch:  13 | Step:  0 | train loss:  0.06505742\n",
      "Epoch:  14 | Step:  0 | train loss:  0.05798924\n",
      "Epoch:  15 | Step:  0 | train loss:  0.05119242\n",
      "Epoch:  16 | Step:  0 | train loss:  0.044720944\n",
      "Epoch:  17 | Step:  0 | train loss:  0.038648065\n",
      "Epoch:  18 | Step:  0 | train loss:  0.03305212\n",
      "Epoch:  19 | Step:  0 | train loss:  0.028006293\n",
      "Epoch:  20 | Step:  0 | train loss:  0.023605762\n",
      "Epoch:  21 | Step:  0 | train loss:  0.019920465\n",
      "Epoch:  22 | Step:  0 | train loss:  0.01699904\n",
      "Epoch:  23 | Step:  0 | train loss:  0.014866494\n",
      "Epoch:  24 | Step:  0 | train loss:  0.013509425\n",
      "Epoch:  25 | Step:  0 | train loss:  0.012854125\n",
      "Epoch:  26 | Step:  0 | train loss:  0.012784708\n",
      "Epoch:  27 | Step:  0 | train loss:  0.013130365\n",
      "Epoch:  28 | Step:  0 | train loss:  0.013690308\n",
      "Epoch:  29 | Step:  0 | train loss:  0.014270735\n",
      "Epoch:  30 | Step:  0 | train loss:  0.014717957\n",
      "Epoch:  31 | Step:  0 | train loss:  0.0149452295\n",
      "Epoch:  32 | Step:  0 | train loss:  0.014917579\n",
      "Epoch:  33 | Step:  0 | train loss:  0.014647388\n",
      "Epoch:  34 | Step:  0 | train loss:  0.014159927\n",
      "Epoch:  35 | Step:  0 | train loss:  0.013494746\n",
      "Epoch:  36 | Step:  0 | train loss:  0.012759418\n",
      "Epoch:  37 | Step:  0 | train loss:  0.011997637\n",
      "Epoch:  38 | Step:  0 | train loss:  0.0112951845\n",
      "Epoch:  39 | Step:  0 | train loss:  0.010699778\n",
      "Epoch:  40 | Step:  0 | train loss:  0.0102516245\n",
      "Epoch:  41 | Step:  0 | train loss:  0.009978796\n",
      "Epoch:  42 | Step:  0 | train loss:  0.009884622\n",
      "Epoch:  43 | Step:  0 | train loss:  0.009953242\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010129556\n",
      "Epoch:  45 | Step:  0 | train loss:  0.01033449\n",
      "Epoch:  46 | Step:  0 | train loss:  0.010498124\n",
      "Epoch:  47 | Step:  0 | train loss:  0.010583707\n",
      "Epoch:  48 | Step:  0 | train loss:  0.010584315\n",
      "Epoch:  49 | Step:  0 | train loss:  0.010513736\n",
      "Epoch:  50 | Step:  0 | train loss:  0.010394039\n",
      "Epoch:  51 | Step:  0 | train loss:  0.010248891\n",
      "Epoch:  52 | Step:  0 | train loss:  0.010099967\n",
      "Epoch:  53 | Step:  0 | train loss:  0.009963434\n",
      "Epoch:  54 | Step:  0 | train loss:  0.009849618\n",
      "Epoch:  55 | Step:  0 | train loss:  0.009762962\n",
      "Epoch:  56 | Step:  0 | train loss:  0.009703302\n",
      "Epoch:  57 | Step:  0 | train loss:  0.00966742\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009650236\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009645892\n",
      "Epoch:  60 | Step:  0 | train loss:  0.0096486695\n",
      "Epoch:  61 | Step:  0 | train loss:  0.009653579\n",
      "Epoch:  62 | Step:  0 | train loss:  0.009656632\n",
      "Epoch:  63 | Step:  0 | train loss:  0.009655114\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009648451\n",
      "Epoch:  65 | Step:  0 | train loss:  0.009624413\n",
      "Epoch:  66 | Step:  0 | train loss:  0.0095958905\n",
      "Epoch:  67 | Step:  0 | train loss:  0.00954657\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009508225\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009498921\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009512827\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009518139\n",
      "Epoch:  72 | Step:  0 | train loss:  0.009503741\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009481568\n",
      "Epoch:  74 | Step:  0 | train loss:  0.009462509\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009456935\n",
      "Epoch:  76 | Step:  0 | train loss:  0.009458502\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009459294\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009455301\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009446208\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009434438\n",
      "Epoch:  81 | Step:  0 | train loss:  0.0094232075\n",
      "Epoch:  82 | Step:  0 | train loss:  0.00941386\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009404621\n",
      "Epoch:  84 | Step:  0 | train loss:  0.009400974\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009395342\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009385116\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009370637\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009358293\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009346308\n",
      "Epoch:  90 | Step:  0 | train loss:  0.0093376385\n",
      "Epoch:  91 | Step:  0 | train loss:  0.009328351\n",
      "Epoch:  92 | Step:  0 | train loss:  0.00931633\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009299104\n",
      "Epoch:  94 | Step:  0 | train loss:  0.009270403\n",
      "Epoch:  95 | Step:  0 | train loss:  0.009236529\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009205921\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009184394\n",
      "Epoch:  98 | Step:  0 | train loss:  0.009160192\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009123093\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009074302\n",
      "Epoch:  101 | Step:  0 | train loss:  0.009032717\n",
      "Epoch:  102 | Step:  0 | train loss:  0.008989869\n",
      "Epoch:  103 | Step:  0 | train loss:  0.008944971\n",
      "Epoch:  104 | Step:  0 | train loss:  0.008892726\n",
      "Epoch:  105 | Step:  0 | train loss:  0.008845255\n",
      "Epoch:  106 | Step:  0 | train loss:  0.008817892\n",
      "Epoch:  107 | Step:  0 | train loss:  0.008790129\n",
      "Epoch:  108 | Step:  0 | train loss:  0.008759876\n",
      "Epoch:  109 | Step:  0 | train loss:  0.008738566\n",
      "Epoch:  110 | Step:  0 | train loss:  0.008693143\n",
      "Epoch:  111 | Step:  0 | train loss:  0.008661203\n",
      "Epoch:  112 | Step:  0 | train loss:  0.008627976\n",
      "Epoch:  113 | Step:  0 | train loss:  0.008598095\n",
      "Epoch:  114 | Step:  0 | train loss:  0.008569038\n",
      "Epoch:  115 | Step:  0 | train loss:  0.008565518\n",
      "Epoch:  116 | Step:  0 | train loss:  0.0085579455\n",
      "Epoch:  117 | Step:  0 | train loss:  0.00854039\n",
      "Epoch:  118 | Step:  0 | train loss:  0.008527296\n",
      "Epoch:  119 | Step:  0 | train loss:  0.008516654\n",
      "Epoch:  120 | Step:  0 | train loss:  0.008508328\n",
      "Epoch:  121 | Step:  0 | train loss:  0.008494895\n",
      "Epoch:  122 | Step:  0 | train loss:  0.008484536\n",
      "Epoch:  123 | Step:  0 | train loss:  0.008474969\n",
      "Epoch:  124 | Step:  0 | train loss:  0.008473221\n",
      "Epoch:  125 | Step:  0 | train loss:  0.008477594\n",
      "Epoch:  126 | Step:  0 | train loss:  0.008454511\n",
      "Epoch:  127 | Step:  0 | train loss:  0.008455216\n",
      "Epoch:  128 | Step:  0 | train loss:  0.008438838\n",
      "Epoch:  129 | Step:  0 | train loss:  0.008435238\n",
      "Epoch:  130 | Step:  0 | train loss:  0.00843011\n",
      "Epoch:  131 | Step:  0 | train loss:  0.008424925\n",
      "Epoch:  132 | Step:  0 | train loss:  0.008413674\n",
      "Epoch:  133 | Step:  0 | train loss:  0.008405604\n",
      "Epoch:  134 | Step:  0 | train loss:  0.008401325\n",
      "Epoch:  135 | Step:  0 | train loss:  0.008398529\n",
      "Epoch:  136 | Step:  0 | train loss:  0.008395355\n",
      "Epoch:  137 | Step:  0 | train loss:  0.008390044\n",
      "Epoch:  138 | Step:  0 | train loss:  0.008383191\n",
      "Epoch:  139 | Step:  0 | train loss:  0.0083604995\n",
      "Epoch:  140 | Step:  0 | train loss:  0.0083511565\n",
      "Epoch:  141 | Step:  0 | train loss:  0.0083267335\n",
      "Epoch:  142 | Step:  0 | train loss:  0.008321018\n",
      "Epoch:  143 | Step:  0 | train loss:  0.008321722\n",
      "Epoch:  144 | Step:  0 | train loss:  0.008305957\n",
      "Epoch:  145 | Step:  0 | train loss:  0.008302711\n",
      "Epoch:  146 | Step:  0 | train loss:  0.008295424\n",
      "Epoch:  147 | Step:  0 | train loss:  0.008293083\n",
      "Epoch:  148 | Step:  0 | train loss:  0.008290499\n",
      "Epoch:  149 | Step:  0 | train loss:  0.008287348\n",
      "Epoch:  150 | Step:  0 | train loss:  0.008283705\n",
      "Epoch:  151 | Step:  0 | train loss:  0.008269018\n",
      "Epoch:  152 | Step:  0 | train loss:  0.008265082\n",
      "Epoch:  153 | Step:  0 | train loss:  0.00825907\n",
      "Epoch:  154 | Step:  0 | train loss:  0.008255829\n",
      "Epoch:  155 | Step:  0 | train loss:  0.008251818\n",
      "Epoch:  156 | Step:  0 | train loss:  0.008238612\n",
      "Epoch:  157 | Step:  0 | train loss:  0.008233883\n",
      "Epoch:  158 | Step:  0 | train loss:  0.008232378\n",
      "Epoch:  159 | Step:  0 | train loss:  0.008228525\n",
      "Epoch:  160 | Step:  0 | train loss:  0.008226121\n",
      "Epoch:  161 | Step:  0 | train loss:  0.008225337\n",
      "Epoch:  162 | Step:  0 | train loss:  0.008219932\n",
      "Epoch:  163 | Step:  0 | train loss:  0.008217821\n",
      "Epoch:  164 | Step:  0 | train loss:  0.008216712\n",
      "Epoch:  165 | Step:  0 | train loss:  0.008214388\n",
      "Epoch:  166 | Step:  0 | train loss:  0.008210154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.008209213\n",
      "Epoch:  168 | Step:  0 | train loss:  0.008206341\n",
      "Epoch:  169 | Step:  0 | train loss:  0.008200842\n",
      "Epoch:  170 | Step:  0 | train loss:  0.00819698\n",
      "Epoch:  171 | Step:  0 | train loss:  0.008192869\n",
      "Epoch:  172 | Step:  0 | train loss:  0.00817636\n",
      "Epoch:  173 | Step:  0 | train loss:  0.008174896\n",
      "Epoch:  174 | Step:  0 | train loss:  0.008172802\n",
      "Epoch:  175 | Step:  0 | train loss:  0.008167594\n",
      "Epoch:  176 | Step:  0 | train loss:  0.008163782\n",
      "Epoch:  177 | Step:  0 | train loss:  0.008159443\n",
      "Epoch:  178 | Step:  0 | train loss:  0.008156802\n",
      "Epoch:  179 | Step:  0 | train loss:  0.008153643\n",
      "Epoch:  180 | Step:  0 | train loss:  0.008151137\n",
      "Epoch:  181 | Step:  0 | train loss:  0.008147767\n",
      "Epoch:  182 | Step:  0 | train loss:  0.0081445\n",
      "Epoch:  183 | Step:  0 | train loss:  0.008138561\n",
      "Epoch:  184 | Step:  0 | train loss:  0.008138697\n",
      "Epoch:  185 | Step:  0 | train loss:  0.008145599\n",
      "Epoch:  186 | Step:  0 | train loss:  0.008137192\n",
      "Epoch:  187 | Step:  0 | train loss:  0.008127728\n",
      "Epoch:  188 | Step:  0 | train loss:  0.008124949\n",
      "Epoch:  189 | Step:  0 | train loss:  0.008129568\n",
      "Epoch:  190 | Step:  0 | train loss:  0.0081544025\n",
      "Epoch:  191 | Step:  0 | train loss:  0.00811923\n",
      "Epoch:  192 | Step:  0 | train loss:  0.008132742\n",
      "Epoch:  193 | Step:  0 | train loss:  0.008133485\n",
      "Epoch:  194 | Step:  0 | train loss:  0.008126605\n",
      "Epoch:  195 | Step:  0 | train loss:  0.00816009\n",
      "Epoch:  196 | Step:  0 | train loss:  0.008106592\n",
      "Epoch:  197 | Step:  0 | train loss:  0.008112205\n",
      "Epoch:  198 | Step:  0 | train loss:  0.008105031\n",
      "Epoch:  199 | Step:  0 | train loss:  0.008110249\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader):          # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.4896e+00,  3.0606e+00,  2.8383e+00,  ...,  3.0138e+00,\n",
       "           2.9828e+00,  4.0948e-01],\n",
       "         [ 3.2191e+00,  2.8453e+00,  2.5168e+00,  ...,  2.7897e+00,\n",
       "           2.7123e+00,  3.3035e-01],\n",
       "         [ 3.1501e+00,  2.7645e+00,  2.4523e+00,  ...,  2.7290e+00,\n",
       "           2.6578e+00,  3.1888e-01],\n",
       "         ...,\n",
       "         [ 3.7096e+00,  3.2644e+00,  3.0586e+00,  ...,  3.1888e+00,\n",
       "           3.1733e+00,  4.5731e-01],\n",
       "         [ 3.7486e+00,  3.3726e+00,  3.0560e+00,  ...,  3.2411e+00,\n",
       "           3.1785e+00,  4.4945e-01],\n",
       "         [ 3.6978e+00,  3.2522e+00,  3.0464e+00,  ...,  3.1782e+00,\n",
       "           3.1628e+00,  4.5508e-01]]], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.2338423891180372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
