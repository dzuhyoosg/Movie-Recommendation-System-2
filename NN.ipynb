{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.003\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 500)\n",
    "        self.fc2 = nn.Linear(500, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.fc4 = nn.Linear(10, 50)\n",
    "        self.fc5 = nn.Linear(50, 500)\n",
    "        self.fc6 = nn.Linear(500, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation_t(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.activation(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.17148888\n",
      "Epoch:  1 | Step:  0 | train loss:  0.11271715\n",
      "Epoch:  2 | Step:  0 | train loss:  0.0663337\n",
      "Epoch:  3 | Step:  0 | train loss:  0.033507507\n",
      "Epoch:  4 | Step:  0 | train loss:  0.015951904\n",
      "Epoch:  5 | Step:  0 | train loss:  0.013501621\n",
      "Epoch:  6 | Step:  0 | train loss:  0.021626445\n",
      "Epoch:  7 | Step:  0 | train loss:  0.031184664\n",
      "Epoch:  8 | Step:  0 | train loss:  0.035927992\n",
      "Epoch:  9 | Step:  0 | train loss:  0.034919977\n",
      "Epoch:  10 | Step:  0 | train loss:  0.030004762\n",
      "Epoch:  11 | Step:  0 | train loss:  0.023699662\n",
      "Epoch:  12 | Step:  0 | train loss:  0.01803793\n",
      "Epoch:  13 | Step:  0 | train loss:  0.014153437\n",
      "Epoch:  14 | Step:  0 | train loss:  0.012317966\n",
      "Epoch:  15 | Step:  0 | train loss:  0.012191646\n",
      "Epoch:  16 | Step:  0 | train loss:  0.013129096\n",
      "Epoch:  17 | Step:  0 | train loss:  0.014441997\n",
      "Epoch:  18 | Step:  0 | train loss:  0.015573674\n",
      "Epoch:  19 | Step:  0 | train loss:  0.016179722\n",
      "Epoch:  20 | Step:  0 | train loss:  0.01613393\n",
      "Epoch:  21 | Step:  0 | train loss:  0.015490872\n",
      "Epoch:  22 | Step:  0 | train loss:  0.014429662\n",
      "Epoch:  23 | Step:  0 | train loss:  0.01319294\n",
      "Epoch:  24 | Step:  0 | train loss:  0.012027964\n",
      "Epoch:  25 | Step:  0 | train loss:  0.011134585\n",
      "Epoch:  26 | Step:  0 | train loss:  0.010626014\n",
      "Epoch:  27 | Step:  0 | train loss:  0.010509664\n",
      "Epoch:  28 | Step:  0 | train loss:  0.010694175\n",
      "Epoch:  29 | Step:  0 | train loss:  0.011023296\n",
      "Epoch:  30 | Step:  0 | train loss:  0.011327656\n",
      "Epoch:  31 | Step:  0 | train loss:  0.011476721\n",
      "Epoch:  32 | Step:  0 | train loss:  0.011412755\n",
      "Epoch:  33 | Step:  0 | train loss:  0.011156672\n",
      "Epoch:  34 | Step:  0 | train loss:  0.010787628\n",
      "Epoch:  35 | Step:  0 | train loss:  0.010407679\n",
      "Epoch:  36 | Step:  0 | train loss:  0.01010596\n",
      "Epoch:  37 | Step:  0 | train loss:  0.009933897\n",
      "Epoch:  38 | Step:  0 | train loss:  0.009896885\n",
      "Epoch:  39 | Step:  0 | train loss:  0.009961407\n",
      "Epoch:  40 | Step:  0 | train loss:  0.010072113\n",
      "Epoch:  41 | Step:  0 | train loss:  0.010171797\n",
      "Epoch:  42 | Step:  0 | train loss:  0.010218071\n",
      "Epoch:  43 | Step:  0 | train loss:  0.01019284\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010103376\n",
      "Epoch:  45 | Step:  0 | train loss:  0.00997611\n",
      "Epoch:  46 | Step:  0 | train loss:  0.009845913\n",
      "Epoch:  47 | Step:  0 | train loss:  0.00974431\n",
      "Epoch:  48 | Step:  0 | train loss:  0.009690087\n",
      "Epoch:  49 | Step:  0 | train loss:  0.009684872\n",
      "Epoch:  50 | Step:  0 | train loss:  0.00971471\n",
      "Epoch:  51 | Step:  0 | train loss:  0.009756701\n",
      "Epoch:  52 | Step:  0 | train loss:  0.009788007\n",
      "Epoch:  53 | Step:  0 | train loss:  0.009793766\n",
      "Epoch:  54 | Step:  0 | train loss:  0.009771115\n",
      "Epoch:  55 | Step:  0 | train loss:  0.009728287\n",
      "Epoch:  56 | Step:  0 | train loss:  0.009679782\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009640018\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009618074\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009615283\n",
      "Epoch:  60 | Step:  0 | train loss:  0.009626085\n",
      "Epoch:  61 | Step:  0 | train loss:  0.009641234\n",
      "Epoch:  62 | Step:  0 | train loss:  0.009651805\n",
      "Epoch:  63 | Step:  0 | train loss:  0.009652401\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009642521\n",
      "Epoch:  65 | Step:  0 | train loss:  0.009625906\n",
      "Epoch:  66 | Step:  0 | train loss:  0.009608411\n",
      "Epoch:  67 | Step:  0 | train loss:  0.009595392\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009589686\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009590868\n",
      "Epoch:  70 | Step:  0 | train loss:  0.0095959315\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009600923\n",
      "Epoch:  72 | Step:  0 | train loss:  0.009602721\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009600201\n",
      "Epoch:  74 | Step:  0 | train loss:  0.009594367\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009587546\n",
      "Epoch:  76 | Step:  0 | train loss:  0.009582148\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009579594\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009579869\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009581769\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009583641\n",
      "Epoch:  81 | Step:  0 | train loss:  0.009584189\n",
      "Epoch:  82 | Step:  0 | train loss:  0.009582992\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009580534\n",
      "Epoch:  84 | Step:  0 | train loss:  0.009577829\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009575861\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009575125\n",
      "Epoch:  87 | Step:  0 | train loss:  0.00957548\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009576313\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009576914\n",
      "Epoch:  90 | Step:  0 | train loss:  0.009576832\n",
      "Epoch:  91 | Step:  0 | train loss:  0.00957605\n",
      "Epoch:  92 | Step:  0 | train loss:  0.009574916\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009573907\n",
      "Epoch:  94 | Step:  0 | train loss:  0.009573365\n",
      "Epoch:  95 | Step:  0 | train loss:  0.009573351\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009573658\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009573966\n",
      "Epoch:  98 | Step:  0 | train loss:  0.009574022\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009573757\n",
      "Epoch:  100 | Step:  0 | train loss:  0.00957329\n",
      "Epoch:  101 | Step:  0 | train loss:  0.009572833\n",
      "Epoch:  102 | Step:  0 | train loss:  0.009572559\n",
      "Epoch:  103 | Step:  0 | train loss:  0.009572521\n",
      "Epoch:  104 | Step:  0 | train loss:  0.00957264\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009572773\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009572799\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009572684\n",
      "Epoch:  108 | Step:  0 | train loss:  0.00957248\n",
      "Epoch:  109 | Step:  0 | train loss:  0.009572283\n",
      "Epoch:  110 | Step:  0 | train loss:  0.009572173\n",
      "Epoch:  111 | Step:  0 | train loss:  0.009572165\n",
      "Epoch:  112 | Step:  0 | train loss:  0.00957222\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009572269\n",
      "Epoch:  114 | Step:  0 | train loss:  0.009572266\n",
      "Epoch:  115 | Step:  0 | train loss:  0.0095722005\n",
      "Epoch:  116 | Step:  0 | train loss:  0.009572106\n",
      "Epoch:  117 | Step:  0 | train loss:  0.009572028\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009571995\n",
      "Epoch:  119 | Step:  0 | train loss:  0.009572002\n",
      "Epoch:  120 | Step:  0 | train loss:  0.009572026\n",
      "Epoch:  121 | Step:  0 | train loss:  0.0095720375\n",
      "Epoch:  122 | Step:  0 | train loss:  0.009572021\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009571983\n",
      "Epoch:  124 | Step:  0 | train loss:  0.009571941\n",
      "Epoch:  125 | Step:  0 | train loss:  0.009571915\n",
      "Epoch:  126 | Step:  0 | train loss:  0.009571909\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009571917\n",
      "Epoch:  128 | Step:  0 | train loss:  0.009571923\n",
      "Epoch:  129 | Step:  0 | train loss:  0.009571917\n",
      "Epoch:  130 | Step:  0 | train loss:  0.0095719\n",
      "Epoch:  131 | Step:  0 | train loss:  0.009571877\n",
      "Epoch:  132 | Step:  0 | train loss:  0.009571861\n",
      "Epoch:  133 | Step:  0 | train loss:  0.009571853\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009571852\n",
      "Epoch:  135 | Step:  0 | train loss:  0.009571851\n",
      "Epoch:  136 | Step:  0 | train loss:  0.009571845\n",
      "Epoch:  137 | Step:  0 | train loss:  0.009571831\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009571814\n",
      "Epoch:  139 | Step:  0 | train loss:  0.009571799\n",
      "Epoch:  140 | Step:  0 | train loss:  0.009571787\n",
      "Epoch:  141 | Step:  0 | train loss:  0.009571777\n",
      "Epoch:  142 | Step:  0 | train loss:  0.009571766\n",
      "Epoch:  143 | Step:  0 | train loss:  0.009571751\n",
      "Epoch:  144 | Step:  0 | train loss:  0.009571732\n",
      "Epoch:  145 | Step:  0 | train loss:  0.009571712\n",
      "Epoch:  146 | Step:  0 | train loss:  0.009571695\n",
      "Epoch:  147 | Step:  0 | train loss:  0.009571679\n",
      "Epoch:  148 | Step:  0 | train loss:  0.009571666\n",
      "Epoch:  149 | Step:  0 | train loss:  0.009571654\n",
      "Epoch:  150 | Step:  0 | train loss:  0.009571639\n",
      "Epoch:  151 | Step:  0 | train loss:  0.009571622\n",
      "Epoch:  152 | Step:  0 | train loss:  0.009571603\n",
      "Epoch:  153 | Step:  0 | train loss:  0.009571581\n",
      "Epoch:  154 | Step:  0 | train loss:  0.009571557\n",
      "Epoch:  155 | Step:  0 | train loss:  0.00957153\n",
      "Epoch:  156 | Step:  0 | train loss:  0.009571499\n",
      "Epoch:  157 | Step:  0 | train loss:  0.009571466\n",
      "Epoch:  158 | Step:  0 | train loss:  0.009571428\n",
      "Epoch:  159 | Step:  0 | train loss:  0.009571391\n",
      "Epoch:  160 | Step:  0 | train loss:  0.009571351\n",
      "Epoch:  161 | Step:  0 | train loss:  0.009571309\n",
      "Epoch:  162 | Step:  0 | train loss:  0.009571262\n",
      "Epoch:  163 | Step:  0 | train loss:  0.009571208\n",
      "Epoch:  164 | Step:  0 | train loss:  0.009571146\n",
      "Epoch:  165 | Step:  0 | train loss:  0.009571075\n",
      "Epoch:  166 | Step:  0 | train loss:  0.009570997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.00957091\n",
      "Epoch:  168 | Step:  0 | train loss:  0.009570809\n",
      "Epoch:  169 | Step:  0 | train loss:  0.00957069\n",
      "Epoch:  170 | Step:  0 | train loss:  0.009570545\n",
      "Epoch:  171 | Step:  0 | train loss:  0.009570365\n",
      "Epoch:  172 | Step:  0 | train loss:  0.009570151\n",
      "Epoch:  173 | Step:  0 | train loss:  0.009569919\n",
      "Epoch:  174 | Step:  0 | train loss:  0.009569702\n",
      "Epoch:  175 | Step:  0 | train loss:  0.009569507\n",
      "Epoch:  176 | Step:  0 | train loss:  0.009569306\n",
      "Epoch:  177 | Step:  0 | train loss:  0.009569061\n",
      "Epoch:  178 | Step:  0 | train loss:  0.0095687425\n",
      "Epoch:  179 | Step:  0 | train loss:  0.009568324\n",
      "Epoch:  180 | Step:  0 | train loss:  0.009567789\n",
      "Epoch:  181 | Step:  0 | train loss:  0.009567177\n",
      "Epoch:  182 | Step:  0 | train loss:  0.009566609\n",
      "Epoch:  183 | Step:  0 | train loss:  0.009566106\n",
      "Epoch:  184 | Step:  0 | train loss:  0.009565519\n",
      "Epoch:  185 | Step:  0 | train loss:  0.00956474\n",
      "Epoch:  186 | Step:  0 | train loss:  0.009563748\n",
      "Epoch:  187 | Step:  0 | train loss:  0.009562582\n",
      "Epoch:  188 | Step:  0 | train loss:  0.009561316\n",
      "Epoch:  189 | Step:  0 | train loss:  0.009559961\n",
      "Epoch:  190 | Step:  0 | train loss:  0.009558343\n",
      "Epoch:  191 | Step:  0 | train loss:  0.009556304\n",
      "Epoch:  192 | Step:  0 | train loss:  0.009553829\n",
      "Epoch:  193 | Step:  0 | train loss:  0.009550932\n",
      "Epoch:  194 | Step:  0 | train loss:  0.009547527\n",
      "Epoch:  195 | Step:  0 | train loss:  0.009543413\n",
      "Epoch:  196 | Step:  0 | train loss:  0.009538401\n",
      "Epoch:  197 | Step:  0 | train loss:  0.009532414\n",
      "Epoch:  198 | Step:  0 | train loss:  0.009525417\n",
      "Epoch:  199 | Step:  0 | train loss:  0.009517275\n",
      "Epoch:  200 | Step:  0 | train loss:  0.009507919\n",
      "Epoch:  201 | Step:  0 | train loss:  0.009497309\n",
      "Epoch:  202 | Step:  0 | train loss:  0.0094855195\n",
      "Epoch:  203 | Step:  0 | train loss:  0.009472484\n",
      "Epoch:  204 | Step:  0 | train loss:  0.009457787\n",
      "Epoch:  205 | Step:  0 | train loss:  0.009441129\n",
      "Epoch:  206 | Step:  0 | train loss:  0.009422421\n",
      "Epoch:  207 | Step:  0 | train loss:  0.00940267\n",
      "Epoch:  208 | Step:  0 | train loss:  0.009383126\n",
      "Epoch:  209 | Step:  0 | train loss:  0.009363785\n",
      "Epoch:  210 | Step:  0 | train loss:  0.009343969\n",
      "Epoch:  211 | Step:  0 | train loss:  0.009322781\n",
      "Epoch:  212 | Step:  0 | train loss:  0.009298966\n",
      "Epoch:  213 | Step:  0 | train loss:  0.009271114\n",
      "Epoch:  214 | Step:  0 | train loss:  0.009238937\n",
      "Epoch:  215 | Step:  0 | train loss:  0.009203301\n",
      "Epoch:  216 | Step:  0 | train loss:  0.0091641545\n",
      "Epoch:  217 | Step:  0 | train loss:  0.009119538\n",
      "Epoch:  218 | Step:  0 | train loss:  0.00907052\n",
      "Epoch:  219 | Step:  0 | train loss:  0.009018839\n",
      "Epoch:  220 | Step:  0 | train loss:  0.0089645805\n",
      "Epoch:  221 | Step:  0 | train loss:  0.0089078\n",
      "Epoch:  222 | Step:  0 | train loss:  0.008849395\n",
      "Epoch:  223 | Step:  0 | train loss:  0.008791749\n",
      "Epoch:  224 | Step:  0 | train loss:  0.008732911\n",
      "Epoch:  225 | Step:  0 | train loss:  0.008682892\n",
      "Epoch:  226 | Step:  0 | train loss:  0.008619112\n",
      "Epoch:  227 | Step:  0 | train loss:  0.008567612\n",
      "Epoch:  228 | Step:  0 | train loss:  0.008511487\n",
      "Epoch:  229 | Step:  0 | train loss:  0.008453672\n",
      "Epoch:  230 | Step:  0 | train loss:  0.008401257\n",
      "Epoch:  231 | Step:  0 | train loss:  0.008347331\n",
      "Epoch:  232 | Step:  0 | train loss:  0.0082872445\n",
      "Epoch:  233 | Step:  0 | train loss:  0.008233941\n",
      "Epoch:  234 | Step:  0 | train loss:  0.008190022\n",
      "Epoch:  235 | Step:  0 | train loss:  0.008147391\n",
      "Epoch:  236 | Step:  0 | train loss:  0.008097981\n",
      "Epoch:  237 | Step:  0 | train loss:  0.0080576325\n",
      "Epoch:  238 | Step:  0 | train loss:  0.0080243945\n",
      "Epoch:  239 | Step:  0 | train loss:  0.007998027\n",
      "Epoch:  240 | Step:  0 | train loss:  0.0079751\n",
      "Epoch:  241 | Step:  0 | train loss:  0.0079449415\n",
      "Epoch:  242 | Step:  0 | train loss:  0.007911374\n",
      "Epoch:  243 | Step:  0 | train loss:  0.007888681\n",
      "Epoch:  244 | Step:  0 | train loss:  0.0078700865\n",
      "Epoch:  245 | Step:  0 | train loss:  0.007846694\n",
      "Epoch:  246 | Step:  0 | train loss:  0.00781944\n",
      "Epoch:  247 | Step:  0 | train loss:  0.0078000273\n",
      "Epoch:  248 | Step:  0 | train loss:  0.007785097\n",
      "Epoch:  249 | Step:  0 | train loss:  0.007764498\n",
      "Runtime: 443.90283250808716seconds\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "st = time.time()\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader): \n",
    "        # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())\n",
    "\n",
    "print('Runtime: ' + str(time.time()-st) + 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.2028e+00,  3.8200e+00,  3.6546e+00,  ..., -1.9895e-01,\n",
       "          -5.2145e-03, -7.2796e-02],\n",
       "         [ 3.3983e+00,  2.9927e+00,  2.8902e+00,  ..., -1.4101e-01,\n",
       "           1.6666e-02, -7.0990e-02],\n",
       "         [ 3.4957e+00,  3.0929e+00,  2.9818e+00,  ..., -1.4780e-01,\n",
       "           1.4445e-02, -7.1610e-02],\n",
       "         ...,\n",
       "         [ 3.9680e+00,  3.5788e+00,  3.4305e+00,  ..., -1.8193e-01,\n",
       "           1.5661e-03, -7.2489e-02],\n",
       "         [ 3.0447e+00,  2.6312e+00,  2.5615e+00,  ..., -1.1732e-01,\n",
       "           2.4653e-02, -6.7558e-02],\n",
       "         [ 3.6230e+00,  3.2237e+00,  3.1028e+00,  ..., -1.5687e-01,\n",
       "           1.0428e-02, -7.1226e-02]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.2137675986770897\n",
      "Autoencoder MAE: 1.4732317835983486\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(get_mae(out, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
