{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 500)\n",
    "        self.fc2 = nn.Linear(500, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.fc4 = nn.Linear(10, 50)\n",
    "        self.fc5 = nn.Linear(50, 500)\n",
    "        self.fc6 = nn.Linear(500, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation_t(self.fc3(x))\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.activation(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.17053525\n",
      "Epoch:  1 | Step:  0 | train loss:  0.029187353\n",
      "Epoch:  2 | Step:  0 | train loss:  0.033140343\n",
      "Epoch:  3 | Step:  0 | train loss:  0.062110014\n",
      "Epoch:  4 | Step:  0 | train loss:  0.045809884\n",
      "Epoch:  5 | Step:  0 | train loss:  0.021727068\n",
      "Epoch:  6 | Step:  0 | train loss:  0.013240434\n",
      "Epoch:  7 | Step:  0 | train loss:  0.018372852\n",
      "Epoch:  8 | Step:  0 | train loss:  0.026094064\n",
      "Epoch:  9 | Step:  0 | train loss:  0.029582659\n",
      "Epoch:  10 | Step:  0 | train loss:  0.027665932\n",
      "Epoch:  11 | Step:  0 | train loss:  0.022026101\n",
      "Epoch:  12 | Step:  0 | train loss:  0.015589271\n",
      "Epoch:  13 | Step:  0 | train loss:  0.011438519\n",
      "Epoch:  14 | Step:  0 | train loss:  0.011396294\n",
      "Epoch:  15 | Step:  0 | train loss:  0.0145651875\n",
      "Epoch:  16 | Step:  0 | train loss:  0.017614761\n",
      "Epoch:  17 | Step:  0 | train loss:  0.017788956\n",
      "Epoch:  18 | Step:  0 | train loss:  0.015178564\n",
      "Epoch:  19 | Step:  0 | train loss:  0.011916213\n",
      "Epoch:  20 | Step:  0 | train loss:  0.010081339\n",
      "Epoch:  21 | Step:  0 | train loss:  0.010335013\n",
      "Epoch:  22 | Step:  0 | train loss:  0.011870296\n",
      "Epoch:  23 | Step:  0 | train loss:  0.013298832\n",
      "Epoch:  24 | Step:  0 | train loss:  0.013677072\n",
      "Epoch:  25 | Step:  0 | train loss:  0.012882903\n",
      "Epoch:  26 | Step:  0 | train loss:  0.011453531\n",
      "Epoch:  27 | Step:  0 | train loss:  0.01020255\n",
      "Epoch:  28 | Step:  0 | train loss:  0.009756156\n",
      "Epoch:  29 | Step:  0 | train loss:  0.010177804\n",
      "Epoch:  30 | Step:  0 | train loss:  0.010931369\n",
      "Epoch:  31 | Step:  0 | train loss:  0.011309321\n",
      "Epoch:  32 | Step:  0 | train loss:  0.011012968\n",
      "Epoch:  33 | Step:  0 | train loss:  0.010330972\n",
      "Epoch:  34 | Step:  0 | train loss:  0.009799333\n",
      "Epoch:  35 | Step:  0 | train loss:  0.009734992\n",
      "Epoch:  36 | Step:  0 | train loss:  0.010044384\n",
      "Epoch:  37 | Step:  0 | train loss:  0.0103824595\n",
      "Epoch:  38 | Step:  0 | train loss:  0.01045488\n",
      "Epoch:  39 | Step:  0 | train loss:  0.010213762\n",
      "Epoch:  40 | Step:  0 | train loss:  0.009847882\n",
      "Epoch:  41 | Step:  0 | train loss:  0.009618182\n",
      "Epoch:  42 | Step:  0 | train loss:  0.009656418\n",
      "Epoch:  43 | Step:  0 | train loss:  0.009867497\n",
      "Epoch:  44 | Step:  0 | train loss:  0.01002393\n",
      "Epoch:  45 | Step:  0 | train loss:  0.009976659\n",
      "Epoch:  46 | Step:  0 | train loss:  0.009777328\n",
      "Epoch:  47 | Step:  0 | train loss:  0.009601661\n",
      "Epoch:  48 | Step:  0 | train loss:  0.009575055\n",
      "Epoch:  49 | Step:  0 | train loss:  0.009675449\n",
      "Epoch:  50 | Step:  0 | train loss:  0.009780723\n",
      "Epoch:  51 | Step:  0 | train loss:  0.009788858\n",
      "Epoch:  52 | Step:  0 | train loss:  0.009697992\n",
      "Epoch:  53 | Step:  0 | train loss:  0.009591526\n",
      "Epoch:  54 | Step:  0 | train loss:  0.009555062\n",
      "Epoch:  55 | Step:  0 | train loss:  0.009600302\n",
      "Epoch:  56 | Step:  0 | train loss:  0.009662584\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009671466\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009619781\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009560929\n",
      "Epoch:  60 | Step:  0 | train loss:  0.009546624\n",
      "Epoch:  61 | Step:  0 | train loss:  0.00957709\n",
      "Epoch:  62 | Step:  0 | train loss:  0.0096100615\n",
      "Epoch:  63 | Step:  0 | train loss:  0.009608604\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009575506\n",
      "Epoch:  65 | Step:  0 | train loss:  0.009544417\n",
      "Epoch:  66 | Step:  0 | train loss:  0.009542355\n",
      "Epoch:  67 | Step:  0 | train loss:  0.009563321\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009578782\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009570714\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009549154\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009536965\n",
      "Epoch:  72 | Step:  0 | train loss:  0.009543268\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009556112\n",
      "Epoch:  74 | Step:  0 | train loss:  0.00955885\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009548682\n",
      "Epoch:  76 | Step:  0 | train loss:  0.009537413\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009536304\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009543642\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009548559\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009544824\n",
      "Epoch:  81 | Step:  0 | train loss:  0.009537312\n",
      "Epoch:  82 | Step:  0 | train loss:  0.009534525\n",
      "Epoch:  83 | Step:  0 | train loss:  0.0095380815\n",
      "Epoch:  84 | Step:  0 | train loss:  0.009542001\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009540978\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009536498\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009533975\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009535629\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009538273\n",
      "Epoch:  90 | Step:  0 | train loss:  0.009538071\n",
      "Epoch:  91 | Step:  0 | train loss:  0.0095354\n",
      "Epoch:  92 | Step:  0 | train loss:  0.009533691\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009534611\n",
      "Epoch:  94 | Step:  0 | train loss:  0.009536255\n",
      "Epoch:  95 | Step:  0 | train loss:  0.009536157\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009534516\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009533503\n",
      "Epoch:  98 | Step:  0 | train loss:  0.009534132\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009535116\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009534934\n",
      "Epoch:  101 | Step:  0 | train loss:  0.009533887\n",
      "Epoch:  102 | Step:  0 | train loss:  0.009533397\n",
      "Epoch:  103 | Step:  0 | train loss:  0.0095339045\n",
      "Epoch:  104 | Step:  0 | train loss:  0.009534436\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009534164\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009533512\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009533364\n",
      "Epoch:  108 | Step:  0 | train loss:  0.009533763\n",
      "Epoch:  109 | Step:  0 | train loss:  0.009533985\n",
      "Epoch:  110 | Step:  0 | train loss:  0.009533686\n",
      "Epoch:  111 | Step:  0 | train loss:  0.009533326\n",
      "Epoch:  112 | Step:  0 | train loss:  0.009533379\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009533647\n",
      "Epoch:  114 | Step:  0 | train loss:  0.009533663\n",
      "Epoch:  115 | Step:  0 | train loss:  0.009533409\n",
      "Epoch:  116 | Step:  0 | train loss:  0.00953327\n",
      "Epoch:  117 | Step:  0 | train loss:  0.009533399\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009533527\n",
      "Epoch:  119 | Step:  0 | train loss:  0.009533437\n",
      "Epoch:  120 | Step:  0 | train loss:  0.009533279\n",
      "Epoch:  121 | Step:  0 | train loss:  0.009533281\n",
      "Epoch:  122 | Step:  0 | train loss:  0.009533393\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009533405\n",
      "Epoch:  124 | Step:  0 | train loss:  0.0095333\n",
      "Epoch:  125 | Step:  0 | train loss:  0.009533247\n",
      "Epoch:  126 | Step:  0 | train loss:  0.009533305\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009533351\n",
      "Epoch:  128 | Step:  0 | train loss:  0.009533304\n",
      "Epoch:  129 | Step:  0 | train loss:  0.009533244\n",
      "Epoch:  130 | Step:  0 | train loss:  0.009533261\n",
      "Epoch:  131 | Step:  0 | train loss:  0.009533305\n",
      "Epoch:  132 | Step:  0 | train loss:  0.009533293\n",
      "Epoch:  133 | Step:  0 | train loss:  0.009533247\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009533242\n",
      "Epoch:  135 | Step:  0 | train loss:  0.009533273\n",
      "Epoch:  136 | Step:  0 | train loss:  0.009533277\n",
      "Epoch:  137 | Step:  0 | train loss:  0.009533248\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009533236\n",
      "Epoch:  139 | Step:  0 | train loss:  0.0095332535\n",
      "Epoch:  140 | Step:  0 | train loss:  0.009533263\n",
      "Epoch:  141 | Step:  0 | train loss:  0.009533245\n",
      "Epoch:  142 | Step:  0 | train loss:  0.009533233\n",
      "Epoch:  143 | Step:  0 | train loss:  0.009533243\n",
      "Epoch:  144 | Step:  0 | train loss:  0.009533252\n",
      "Epoch:  145 | Step:  0 | train loss:  0.009533242\n",
      "Epoch:  146 | Step:  0 | train loss:  0.009533232\n",
      "Epoch:  147 | Step:  0 | train loss:  0.009533238\n",
      "Epoch:  148 | Step:  0 | train loss:  0.009533244\n",
      "Epoch:  149 | Step:  0 | train loss:  0.009533239\n",
      "Epoch:  150 | Step:  0 | train loss:  0.009533232\n",
      "Epoch:  151 | Step:  0 | train loss:  0.009533235\n",
      "Epoch:  152 | Step:  0 | train loss:  0.0095332395\n",
      "Epoch:  153 | Step:  0 | train loss:  0.009533236\n",
      "Epoch:  154 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  155 | Step:  0 | train loss:  0.009533234\n",
      "Epoch:  156 | Step:  0 | train loss:  0.009533237\n",
      "Epoch:  157 | Step:  0 | train loss:  0.009533234\n",
      "Epoch:  158 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  159 | Step:  0 | train loss:  0.009533233\n",
      "Epoch:  160 | Step:  0 | train loss:  0.009533235\n",
      "Epoch:  161 | Step:  0 | train loss:  0.009533232\n",
      "Epoch:  162 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  163 | Step:  0 | train loss:  0.009533232\n",
      "Epoch:  164 | Step:  0 | train loss:  0.009533233\n",
      "Epoch:  165 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  166 | Step:  0 | train loss:  0.00953323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.009533232\n",
      "Epoch:  168 | Step:  0 | train loss:  0.009533232\n",
      "Epoch:  169 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  170 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  171 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  172 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  173 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  174 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  175 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  176 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  177 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  178 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  179 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  180 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  181 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  182 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  183 | Step:  0 | train loss:  0.009533231\n",
      "Epoch:  184 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  185 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  186 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  187 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  188 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  189 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  190 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  191 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  192 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  193 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  194 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  195 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  196 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  197 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  198 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  199 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  200 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  201 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  202 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  203 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  204 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  205 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  206 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  207 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  208 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  209 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  210 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  211 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  212 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  213 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  214 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  215 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  216 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  217 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  218 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  219 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  220 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  221 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  222 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  223 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  224 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  225 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  226 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  227 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  228 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  229 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  230 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  231 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  232 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  233 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  234 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  235 | Step:  0 | train loss:  0.00953323\n",
      "Epoch:  236 | Step:  0 | train loss:  0.00953323\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "st = time.time()\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader): \n",
    "        # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())\n",
    "\n",
    "print('Runtime: ' + str(time.time()-st) + 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(get_mae(out, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
