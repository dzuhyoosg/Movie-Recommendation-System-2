{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.1723583\n",
      "Epoch:  1 | Step:  0 | train loss:  0.16284505\n",
      "Epoch:  2 | Step:  0 | train loss:  0.15371983\n",
      "Epoch:  3 | Step:  0 | train loss:  0.14454582\n",
      "Epoch:  4 | Step:  0 | train loss:  0.13526487\n",
      "Epoch:  5 | Step:  0 | train loss:  0.1258189\n",
      "Epoch:  6 | Step:  0 | train loss:  0.11621912\n",
      "Epoch:  7 | Step:  0 | train loss:  0.10651051\n",
      "Epoch:  8 | Step:  0 | train loss:  0.09676116\n",
      "Epoch:  9 | Step:  0 | train loss:  0.087046176\n",
      "Epoch:  10 | Step:  0 | train loss:  0.077450715\n",
      "Epoch:  11 | Step:  0 | train loss:  0.06808608\n",
      "Epoch:  12 | Step:  0 | train loss:  0.05907274\n",
      "Epoch:  13 | Step:  0 | train loss:  0.05055981\n",
      "Epoch:  14 | Step:  0 | train loss:  0.04269665\n",
      "Epoch:  15 | Step:  0 | train loss:  0.035631064\n",
      "Epoch:  16 | Step:  0 | train loss:  0.029491697\n",
      "Epoch:  17 | Step:  0 | train loss:  0.024377426\n",
      "Epoch:  18 | Step:  0 | train loss:  0.020307442\n",
      "Epoch:  19 | Step:  0 | train loss:  0.017249169\n",
      "Epoch:  20 | Step:  0 | train loss:  0.015114124\n",
      "Epoch:  21 | Step:  0 | train loss:  0.013789122\n",
      "Epoch:  22 | Step:  0 | train loss:  0.013124925\n",
      "Epoch:  23 | Step:  0 | train loss:  0.012963389\n",
      "Epoch:  24 | Step:  0 | train loss:  0.013145168\n",
      "Epoch:  25 | Step:  0 | train loss:  0.013522422\n",
      "Epoch:  26 | Step:  0 | train loss:  0.013968533\n",
      "Epoch:  27 | Step:  0 | train loss:  0.014383689\n",
      "Epoch:  28 | Step:  0 | train loss:  0.014697066\n",
      "Epoch:  29 | Step:  0 | train loss:  0.014866013\n",
      "Epoch:  30 | Step:  0 | train loss:  0.014872971\n",
      "Epoch:  31 | Step:  0 | train loss:  0.014720953\n",
      "Epoch:  32 | Step:  0 | train loss:  0.014428519\n",
      "Epoch:  33 | Step:  0 | train loss:  0.014024729\n",
      "Epoch:  34 | Step:  0 | train loss:  0.0135443425\n",
      "Epoch:  35 | Step:  0 | train loss:  0.013023619\n",
      "Epoch:  36 | Step:  0 | train loss:  0.012496892\n",
      "Epoch:  37 | Step:  0 | train loss:  0.011994002\n",
      "Epoch:  38 | Step:  0 | train loss:  0.01153855\n",
      "Epoch:  39 | Step:  0 | train loss:  0.011147002\n",
      "Epoch:  40 | Step:  0 | train loss:  0.010828529\n",
      "Epoch:  41 | Step:  0 | train loss:  0.010585459\n",
      "Epoch:  42 | Step:  0 | train loss:  0.010414183\n",
      "Epoch:  43 | Step:  0 | train loss:  0.010306373\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010250375\n",
      "Epoch:  45 | Step:  0 | train loss:  0.010232659\n",
      "Epoch:  46 | Step:  0 | train loss:  0.010239222\n",
      "Epoch:  47 | Step:  0 | train loss:  0.010256839\n",
      "Epoch:  48 | Step:  0 | train loss:  0.010274094\n",
      "Epoch:  49 | Step:  0 | train loss:  0.010282131\n",
      "Epoch:  50 | Step:  0 | train loss:  0.010275088\n",
      "Epoch:  51 | Step:  0 | train loss:  0.010250171\n",
      "Epoch:  52 | Step:  0 | train loss:  0.010207419\n",
      "Epoch:  53 | Step:  0 | train loss:  0.01014927\n",
      "Epoch:  54 | Step:  0 | train loss:  0.010079902\n",
      "Epoch:  55 | Step:  0 | train loss:  0.010004503\n",
      "Epoch:  56 | Step:  0 | train loss:  0.009928523\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009857004\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009794056\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009742485\n",
      "Epoch:  60 | Step:  0 | train loss:  0.009703733\n",
      "Epoch:  61 | Step:  0 | train loss:  0.00967773\n",
      "Epoch:  62 | Step:  0 | train loss:  0.009663131\n",
      "Epoch:  63 | Step:  0 | train loss:  0.009657665\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009658532\n",
      "Epoch:  65 | Step:  0 | train loss:  0.009662794\n",
      "Epoch:  66 | Step:  0 | train loss:  0.009667802\n",
      "Epoch:  67 | Step:  0 | train loss:  0.009671408\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009672075\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009669038\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009662282\n",
      "Epoch:  71 | Step:  0 | train loss:  0.0096523445\n",
      "Epoch:  72 | Step:  0 | train loss:  0.009640137\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009626832\n",
      "Epoch:  74 | Step:  0 | train loss:  0.009613624\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009601517\n",
      "Epoch:  76 | Step:  0 | train loss:  0.009591247\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009583231\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009577546\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009573958\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009572011\n",
      "Epoch:  81 | Step:  0 | train loss:  0.009571252\n",
      "Epoch:  82 | Step:  0 | train loss:  0.009571138\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009571086\n",
      "Epoch:  84 | Step:  0 | train loss:  0.009570667\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009569577\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009567641\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009564998\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009561834\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009558214\n",
      "Epoch:  90 | Step:  0 | train loss:  0.009554374\n",
      "Epoch:  91 | Step:  0 | train loss:  0.00955068\n",
      "Epoch:  92 | Step:  0 | train loss:  0.009547556\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009544841\n",
      "Epoch:  94 | Step:  0 | train loss:  0.009542502\n",
      "Epoch:  95 | Step:  0 | train loss:  0.00954048\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009538672\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009536957\n",
      "Epoch:  98 | Step:  0 | train loss:  0.00953521\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009533322\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009531216\n",
      "Epoch:  101 | Step:  0 | train loss:  0.009528845\n",
      "Epoch:  102 | Step:  0 | train loss:  0.009526166\n",
      "Epoch:  103 | Step:  0 | train loss:  0.00952308\n",
      "Epoch:  104 | Step:  0 | train loss:  0.00951987\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009516751\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009513537\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009510248\n",
      "Epoch:  108 | Step:  0 | train loss:  0.00950708\n",
      "Epoch:  109 | Step:  0 | train loss:  0.009503901\n",
      "Epoch:  110 | Step:  0 | train loss:  0.009500594\n",
      "Epoch:  111 | Step:  0 | train loss:  0.0094971135\n",
      "Epoch:  112 | Step:  0 | train loss:  0.009493616\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009489883\n",
      "Epoch:  114 | Step:  0 | train loss:  0.0094854785\n",
      "Epoch:  115 | Step:  0 | train loss:  0.009480806\n",
      "Epoch:  116 | Step:  0 | train loss:  0.009476144\n",
      "Epoch:  117 | Step:  0 | train loss:  0.009471293\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009465564\n",
      "Epoch:  119 | Step:  0 | train loss:  0.009460145\n",
      "Epoch:  120 | Step:  0 | train loss:  0.009453928\n",
      "Epoch:  121 | Step:  0 | train loss:  0.009447374\n",
      "Epoch:  122 | Step:  0 | train loss:  0.009440794\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009432764\n",
      "Epoch:  124 | Step:  0 | train loss:  0.009425794\n",
      "Epoch:  125 | Step:  0 | train loss:  0.009419019\n",
      "Epoch:  126 | Step:  0 | train loss:  0.00941142\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009399767\n",
      "Epoch:  128 | Step:  0 | train loss:  0.009388\n",
      "Epoch:  129 | Step:  0 | train loss:  0.009375251\n",
      "Epoch:  130 | Step:  0 | train loss:  0.009362318\n",
      "Epoch:  131 | Step:  0 | train loss:  0.009349512\n",
      "Epoch:  132 | Step:  0 | train loss:  0.009335064\n",
      "Epoch:  133 | Step:  0 | train loss:  0.009317282\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009301757\n",
      "Epoch:  135 | Step:  0 | train loss:  0.009288836\n",
      "Epoch:  136 | Step:  0 | train loss:  0.009276357\n",
      "Epoch:  137 | Step:  0 | train loss:  0.009263289\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009251844\n",
      "Epoch:  139 | Step:  0 | train loss:  0.009238822\n",
      "Epoch:  140 | Step:  0 | train loss:  0.009224491\n",
      "Epoch:  141 | Step:  0 | train loss:  0.00920402\n",
      "Epoch:  142 | Step:  0 | train loss:  0.009189335\n",
      "Epoch:  143 | Step:  0 | train loss:  0.009175921\n",
      "Epoch:  144 | Step:  0 | train loss:  0.009163285\n",
      "Epoch:  145 | Step:  0 | train loss:  0.009148789\n",
      "Epoch:  146 | Step:  0 | train loss:  0.009131627\n",
      "Epoch:  147 | Step:  0 | train loss:  0.009109351\n",
      "Epoch:  148 | Step:  0 | train loss:  0.009090294\n",
      "Epoch:  149 | Step:  0 | train loss:  0.009073926\n",
      "Epoch:  150 | Step:  0 | train loss:  0.009053915\n",
      "Epoch:  151 | Step:  0 | train loss:  0.009038842\n",
      "Epoch:  152 | Step:  0 | train loss:  0.009020712\n",
      "Epoch:  153 | Step:  0 | train loss:  0.009001745\n",
      "Epoch:  154 | Step:  0 | train loss:  0.008986055\n",
      "Epoch:  155 | Step:  0 | train loss:  0.00895882\n",
      "Epoch:  156 | Step:  0 | train loss:  0.008945147\n",
      "Epoch:  157 | Step:  0 | train loss:  0.008931424\n",
      "Epoch:  158 | Step:  0 | train loss:  0.008918031\n",
      "Epoch:  159 | Step:  0 | train loss:  0.00890502\n",
      "Epoch:  160 | Step:  0 | train loss:  0.008891946\n",
      "Epoch:  161 | Step:  0 | train loss:  0.008878945\n",
      "Epoch:  162 | Step:  0 | train loss:  0.00886382\n",
      "Epoch:  163 | Step:  0 | train loss:  0.00885024\n",
      "Epoch:  164 | Step:  0 | train loss:  0.008835684\n",
      "Epoch:  165 | Step:  0 | train loss:  0.008810405\n",
      "Epoch:  166 | Step:  0 | train loss:  0.008796674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.008764559\n",
      "Epoch:  168 | Step:  0 | train loss:  0.008750137\n",
      "Epoch:  169 | Step:  0 | train loss:  0.008737882\n",
      "Epoch:  170 | Step:  0 | train loss:  0.008712377\n",
      "Epoch:  171 | Step:  0 | train loss:  0.008705175\n",
      "Epoch:  172 | Step:  0 | train loss:  0.008667415\n",
      "Epoch:  173 | Step:  0 | train loss:  0.008645235\n",
      "Epoch:  174 | Step:  0 | train loss:  0.008633391\n",
      "Epoch:  175 | Step:  0 | train loss:  0.008617702\n",
      "Epoch:  176 | Step:  0 | train loss:  0.00860562\n",
      "Epoch:  177 | Step:  0 | train loss:  0.008592694\n",
      "Epoch:  178 | Step:  0 | train loss:  0.0085784625\n",
      "Epoch:  179 | Step:  0 | train loss:  0.00856883\n",
      "Epoch:  180 | Step:  0 | train loss:  0.008558336\n",
      "Epoch:  181 | Step:  0 | train loss:  0.008548065\n",
      "Epoch:  182 | Step:  0 | train loss:  0.008537876\n",
      "Epoch:  183 | Step:  0 | train loss:  0.008527402\n",
      "Epoch:  184 | Step:  0 | train loss:  0.008516761\n",
      "Epoch:  185 | Step:  0 | train loss:  0.008506684\n",
      "Epoch:  186 | Step:  0 | train loss:  0.008496743\n",
      "Epoch:  187 | Step:  0 | train loss:  0.008484904\n",
      "Epoch:  188 | Step:  0 | train loss:  0.008474298\n",
      "Epoch:  189 | Step:  0 | train loss:  0.00846868\n",
      "Epoch:  190 | Step:  0 | train loss:  0.008457335\n",
      "Epoch:  191 | Step:  0 | train loss:  0.00844707\n",
      "Epoch:  192 | Step:  0 | train loss:  0.008436993\n",
      "Epoch:  193 | Step:  0 | train loss:  0.008425465\n",
      "Epoch:  194 | Step:  0 | train loss:  0.008411481\n",
      "Epoch:  195 | Step:  0 | train loss:  0.008396522\n",
      "Epoch:  196 | Step:  0 | train loss:  0.008383189\n",
      "Epoch:  197 | Step:  0 | train loss:  0.008365844\n",
      "Epoch:  198 | Step:  0 | train loss:  0.0083549945\n",
      "Epoch:  199 | Step:  0 | train loss:  0.0083342455\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader): \n",
    "        # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.7162, 3.1888, 2.9771,  ..., 3.0120, 3.1203, 3.9134],\n",
       "         [4.1860, 3.6123, 3.4825,  ..., 3.4452, 3.4573, 4.5200],\n",
       "         [4.1594, 3.5873, 3.4581,  ..., 3.4188, 3.4363, 4.4928],\n",
       "         ...,\n",
       "         [4.0424, 3.4853, 3.3263,  ..., 3.3155, 3.3572, 4.3315],\n",
       "         [3.8762, 3.3319, 3.1525,  ..., 3.1566, 3.2341, 4.1315],\n",
       "         [3.9696, 3.4200, 3.2480,  ..., 3.2486, 3.3055, 4.2374]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.211782239605736\n",
      "Autoencoder MAE: 1.4684161962238933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(get_mae(out, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
