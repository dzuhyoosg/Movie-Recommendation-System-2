{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 50)\n",
    "        self.fc4 = nn.Linear(50, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.1702916\n",
      "Epoch:  1 | Step:  0 | train loss:  0.14813638\n",
      "Epoch:  2 | Step:  0 | train loss:  0.12692446\n",
      "Epoch:  3 | Step:  0 | train loss:  0.10671682\n",
      "Epoch:  4 | Step:  0 | train loss:  0.087602094\n",
      "Epoch:  5 | Step:  0 | train loss:  0.06969054\n",
      "Epoch:  6 | Step:  0 | train loss:  0.053416368\n",
      "Epoch:  7 | Step:  0 | train loss:  0.0392434\n",
      "Epoch:  8 | Step:  0 | train loss:  0.027750334\n",
      "Epoch:  9 | Step:  0 | train loss:  0.01944416\n",
      "Epoch:  10 | Step:  0 | train loss:  0.014609699\n",
      "Epoch:  11 | Step:  0 | train loss:  0.013134861\n",
      "Epoch:  12 | Step:  0 | train loss:  0.014381994\n",
      "Epoch:  13 | Step:  0 | train loss:  0.017197138\n",
      "Epoch:  14 | Step:  0 | train loss:  0.020242684\n",
      "Epoch:  15 | Step:  0 | train loss:  0.022478366\n",
      "Epoch:  16 | Step:  0 | train loss:  0.023387387\n",
      "Epoch:  17 | Step:  0 | train loss:  0.0229214\n",
      "Epoch:  18 | Step:  0 | train loss:  0.021348404\n",
      "Epoch:  19 | Step:  0 | train loss:  0.01909379\n",
      "Epoch:  20 | Step:  0 | train loss:  0.016625766\n",
      "Epoch:  21 | Step:  0 | train loss:  0.014358318\n",
      "Epoch:  22 | Step:  0 | train loss:  0.012591648\n",
      "Epoch:  23 | Step:  0 | train loss:  0.011454641\n",
      "Epoch:  24 | Step:  0 | train loss:  0.010931946\n",
      "Epoch:  25 | Step:  0 | train loss:  0.010903128\n",
      "Epoch:  26 | Step:  0 | train loss:  0.011209363\n",
      "Epoch:  27 | Step:  0 | train loss:  0.0116795525\n",
      "Epoch:  28 | Step:  0 | train loss:  0.01215516\n",
      "Epoch:  29 | Step:  0 | train loss:  0.0125107765\n",
      "Epoch:  30 | Step:  0 | train loss:  0.0126670655\n",
      "Epoch:  31 | Step:  0 | train loss:  0.012594777\n",
      "Epoch:  32 | Step:  0 | train loss:  0.012310354\n",
      "Epoch:  33 | Step:  0 | train loss:  0.011866224\n",
      "Epoch:  34 | Step:  0 | train loss:  0.011336758\n",
      "Epoch:  35 | Step:  0 | train loss:  0.010804075\n",
      "Epoch:  36 | Step:  0 | train loss:  0.0103435265\n",
      "Epoch:  37 | Step:  0 | train loss:  0.01001124\n",
      "Epoch:  38 | Step:  0 | train loss:  0.009835047\n",
      "Epoch:  39 | Step:  0 | train loss:  0.009810866\n",
      "Epoch:  40 | Step:  0 | train loss:  0.009905574\n",
      "Epoch:  41 | Step:  0 | train loss:  0.01006622\n",
      "Epoch:  42 | Step:  0 | train loss:  0.010233575\n",
      "Epoch:  43 | Step:  0 | train loss:  0.010356475\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010402872\n",
      "Epoch:  45 | Step:  0 | train loss:  0.01036481\n",
      "Epoch:  46 | Step:  0 | train loss:  0.010256605\n",
      "Epoch:  47 | Step:  0 | train loss:  0.010107716\n",
      "Epoch:  48 | Step:  0 | train loss:  0.009952974\n",
      "Epoch:  49 | Step:  0 | train loss:  0.009823147\n",
      "Epoch:  50 | Step:  0 | train loss:  0.009738164\n",
      "Epoch:  51 | Step:  0 | train loss:  0.009704173\n",
      "Epoch:  52 | Step:  0 | train loss:  0.00971447\n",
      "Epoch:  53 | Step:  0 | train loss:  0.009753358\n",
      "Epoch:  54 | Step:  0 | train loss:  0.009801472\n",
      "Epoch:  55 | Step:  0 | train loss:  0.009841043\n",
      "Epoch:  56 | Step:  0 | train loss:  0.009859863\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009853266\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009823999\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009780361\n",
      "Epoch:  60 | Step:  0 | train loss:  0.0097332755\n",
      "Epoch:  61 | Step:  0 | train loss:  0.009693133\n",
      "Epoch:  62 | Step:  0 | train loss:  0.009667164\n",
      "Epoch:  63 | Step:  0 | train loss:  0.009657947\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009663323\n",
      "Epoch:  65 | Step:  0 | train loss:  0.009677609\n",
      "Epoch:  66 | Step:  0 | train loss:  0.009693657\n",
      "Epoch:  67 | Step:  0 | train loss:  0.00970508\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009707979\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009701701\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009688521\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009672499\n",
      "Epoch:  72 | Step:  0 | train loss:  0.009657967\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009648172\n",
      "Epoch:  74 | Step:  0 | train loss:  0.0096444655\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009646196\n",
      "Epoch:  76 | Step:  0 | train loss:  0.009651235\n",
      "Epoch:  77 | Step:  0 | train loss:  0.0096568605\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009660685\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009661332\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009658695\n",
      "Epoch:  81 | Step:  0 | train loss:  0.009653769\n",
      "Epoch:  82 | Step:  0 | train loss:  0.0096481545\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009643447\n",
      "Epoch:  84 | Step:  0 | train loss:  0.009640722\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009640255\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009641555\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009643636\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009645433\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009646175\n",
      "Epoch:  90 | Step:  0 | train loss:  0.0096456045\n",
      "Epoch:  91 | Step:  0 | train loss:  0.009643981\n",
      "Epoch:  92 | Step:  0 | train loss:  0.009641911\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009640071\n",
      "Epoch:  94 | Step:  0 | train loss:  0.009638953\n",
      "Epoch:  95 | Step:  0 | train loss:  0.00963872\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009639197\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009639991\n",
      "Epoch:  98 | Step:  0 | train loss:  0.009640669\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009640922\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009640659\n",
      "Epoch:  101 | Step:  0 | train loss:  0.009640004\n",
      "Epoch:  102 | Step:  0 | train loss:  0.009639214\n",
      "Epoch:  103 | Step:  0 | train loss:  0.009638561\n",
      "Epoch:  104 | Step:  0 | train loss:  0.009638221\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009638227\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009638473\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009638783\n",
      "Epoch:  108 | Step:  0 | train loss:  0.009638986\n",
      "Epoch:  109 | Step:  0 | train loss:  0.0096389875\n",
      "Epoch:  110 | Step:  0 | train loss:  0.009638795\n",
      "Epoch:  111 | Step:  0 | train loss:  0.009638493\n",
      "Epoch:  112 | Step:  0 | train loss:  0.009638201\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009638017\n",
      "Epoch:  114 | Step:  0 | train loss:  0.009637977\n",
      "Epoch:  115 | Step:  0 | train loss:  0.009638054\n",
      "Epoch:  116 | Step:  0 | train loss:  0.009638178\n",
      "Epoch:  117 | Step:  0 | train loss:  0.009638273\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009638289\n",
      "Epoch:  119 | Step:  0 | train loss:  0.00963822\n",
      "Epoch:  120 | Step:  0 | train loss:  0.009638099\n",
      "Epoch:  121 | Step:  0 | train loss:  0.009637977\n",
      "Epoch:  122 | Step:  0 | train loss:  0.009637897\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009637877\n",
      "Epoch:  124 | Step:  0 | train loss:  0.009637907\n",
      "Epoch:  125 | Step:  0 | train loss:  0.0096379565\n",
      "Epoch:  126 | Step:  0 | train loss:  0.009637992\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009637992\n",
      "Epoch:  128 | Step:  0 | train loss:  0.0096379565\n",
      "Epoch:  129 | Step:  0 | train loss:  0.0096379025\n",
      "Epoch:  130 | Step:  0 | train loss:  0.009637852\n",
      "Epoch:  131 | Step:  0 | train loss:  0.009637822\n",
      "Epoch:  132 | Step:  0 | train loss:  0.00963782\n",
      "Epoch:  133 | Step:  0 | train loss:  0.0096378345\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009637852\n",
      "Epoch:  135 | Step:  0 | train loss:  0.00963786\n",
      "Epoch:  136 | Step:  0 | train loss:  0.00963785\n",
      "Epoch:  137 | Step:  0 | train loss:  0.009637828\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009637802\n",
      "Epoch:  139 | Step:  0 | train loss:  0.009637781\n",
      "Epoch:  140 | Step:  0 | train loss:  0.009637772\n",
      "Epoch:  141 | Step:  0 | train loss:  0.009637773\n",
      "Epoch:  142 | Step:  0 | train loss:  0.009637778\n",
      "Epoch:  143 | Step:  0 | train loss:  0.0096377805\n",
      "Epoch:  144 | Step:  0 | train loss:  0.009637776\n",
      "Epoch:  145 | Step:  0 | train loss:  0.009637765\n",
      "Epoch:  146 | Step:  0 | train loss:  0.00963775\n",
      "Epoch:  147 | Step:  0 | train loss:  0.009637736\n",
      "Epoch:  148 | Step:  0 | train loss:  0.0096377265\n",
      "Epoch:  149 | Step:  0 | train loss:  0.009637721\n",
      "Epoch:  150 | Step:  0 | train loss:  0.009637719\n",
      "Epoch:  151 | Step:  0 | train loss:  0.009637715\n",
      "Epoch:  152 | Step:  0 | train loss:  0.009637708\n",
      "Epoch:  153 | Step:  0 | train loss:  0.009637698\n",
      "Epoch:  154 | Step:  0 | train loss:  0.009637685\n",
      "Epoch:  155 | Step:  0 | train loss:  0.0096376715\n",
      "Epoch:  156 | Step:  0 | train loss:  0.00963766\n",
      "Epoch:  157 | Step:  0 | train loss:  0.00963765\n",
      "Epoch:  158 | Step:  0 | train loss:  0.009637642\n",
      "Epoch:  159 | Step:  0 | train loss:  0.0096376315\n",
      "Epoch:  160 | Step:  0 | train loss:  0.00963762\n",
      "Epoch:  161 | Step:  0 | train loss:  0.009637606\n",
      "Epoch:  162 | Step:  0 | train loss:  0.009637591\n",
      "Epoch:  163 | Step:  0 | train loss:  0.009637575\n",
      "Epoch:  164 | Step:  0 | train loss:  0.009637559\n",
      "Epoch:  165 | Step:  0 | train loss:  0.009637542\n",
      "Epoch:  166 | Step:  0 | train loss:  0.009637524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.009637507\n",
      "Epoch:  168 | Step:  0 | train loss:  0.009637487\n",
      "Epoch:  169 | Step:  0 | train loss:  0.009637466\n",
      "Epoch:  170 | Step:  0 | train loss:  0.009637444\n",
      "Epoch:  171 | Step:  0 | train loss:  0.009637423\n",
      "Epoch:  172 | Step:  0 | train loss:  0.0096374005\n",
      "Epoch:  173 | Step:  0 | train loss:  0.009637378\n",
      "Epoch:  174 | Step:  0 | train loss:  0.009637355\n",
      "Epoch:  175 | Step:  0 | train loss:  0.009637331\n",
      "Epoch:  176 | Step:  0 | train loss:  0.009637305\n",
      "Epoch:  177 | Step:  0 | train loss:  0.009637278\n",
      "Epoch:  178 | Step:  0 | train loss:  0.009637249\n",
      "Epoch:  179 | Step:  0 | train loss:  0.00963722\n",
      "Epoch:  180 | Step:  0 | train loss:  0.009637188\n",
      "Epoch:  181 | Step:  0 | train loss:  0.009637156\n",
      "Epoch:  182 | Step:  0 | train loss:  0.009637121\n",
      "Epoch:  183 | Step:  0 | train loss:  0.009637084\n",
      "Epoch:  184 | Step:  0 | train loss:  0.009637046\n",
      "Epoch:  185 | Step:  0 | train loss:  0.009637005\n",
      "Epoch:  186 | Step:  0 | train loss:  0.009636963\n",
      "Epoch:  187 | Step:  0 | train loss:  0.009636918\n",
      "Epoch:  188 | Step:  0 | train loss:  0.009636872\n",
      "Epoch:  189 | Step:  0 | train loss:  0.009636824\n",
      "Epoch:  190 | Step:  0 | train loss:  0.009636774\n",
      "Epoch:  191 | Step:  0 | train loss:  0.00963672\n",
      "Epoch:  192 | Step:  0 | train loss:  0.009636662\n",
      "Epoch:  193 | Step:  0 | train loss:  0.009636601\n",
      "Epoch:  194 | Step:  0 | train loss:  0.009636535\n",
      "Epoch:  195 | Step:  0 | train loss:  0.009636464\n",
      "Epoch:  196 | Step:  0 | train loss:  0.009636385\n",
      "Epoch:  197 | Step:  0 | train loss:  0.009636298\n",
      "Epoch:  198 | Step:  0 | train loss:  0.0096362\n",
      "Epoch:  199 | Step:  0 | train loss:  0.009636088\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader): \n",
    "        # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.9552, 3.4565, 3.4023,  ..., 3.4995, 3.4986, 3.9993],\n",
       "         [3.9544, 3.4603, 3.4020,  ..., 3.5002, 3.5023, 4.0024],\n",
       "         [3.9546, 3.4570, 3.4019,  ..., 3.4995, 3.4992, 3.9995],\n",
       "         ...,\n",
       "         [3.9549, 3.4557, 3.4021,  ..., 3.4993, 3.4979, 3.9985],\n",
       "         [3.9553, 3.4571, 3.4025,  ..., 3.4997, 3.4993, 4.0000],\n",
       "         [3.9553, 3.4572, 3.4025,  ..., 3.4998, 3.4993, 4.0000]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.1815906639893674\n",
      "Autoencoder MAE: 1.3961564972268343\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(get_mae(out, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
