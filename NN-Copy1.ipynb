{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.003\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80668, 20168)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.2)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 50)\n",
    "        self.fc4 = nn.Linear(50, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.18258794\n",
      "Epoch:  1 | Step:  0 | train loss:  0.17523335\n",
      "Epoch:  2 | Step:  0 | train loss:  0.1681198\n",
      "Epoch:  3 | Step:  0 | train loss:  0.16097434\n",
      "Epoch:  4 | Step:  0 | train loss:  0.15376289\n",
      "Epoch:  5 | Step:  0 | train loss:  0.14644799\n",
      "Epoch:  6 | Step:  0 | train loss:  0.13906607\n",
      "Epoch:  7 | Step:  0 | train loss:  0.13164444\n",
      "Epoch:  8 | Step:  0 | train loss:  0.12428983\n",
      "Epoch:  9 | Step:  0 | train loss:  0.11702735\n",
      "Epoch:  10 | Step:  0 | train loss:  0.10991522\n",
      "Epoch:  11 | Step:  0 | train loss:  0.102915905\n",
      "Epoch:  12 | Step:  0 | train loss:  0.09603938\n",
      "Epoch:  13 | Step:  0 | train loss:  0.08931751\n",
      "Epoch:  14 | Step:  0 | train loss:  0.082773164\n",
      "Epoch:  15 | Step:  0 | train loss:  0.07642961\n",
      "Epoch:  16 | Step:  0 | train loss:  0.070308\n",
      "Epoch:  17 | Step:  0 | train loss:  0.06443305\n",
      "Epoch:  18 | Step:  0 | train loss:  0.058825426\n",
      "Epoch:  19 | Step:  0 | train loss:  0.05350514\n",
      "Epoch:  20 | Step:  0 | train loss:  0.048492875\n",
      "Epoch:  21 | Step:  0 | train loss:  0.043804295\n",
      "Epoch:  22 | Step:  0 | train loss:  0.03945263\n",
      "Epoch:  23 | Step:  0 | train loss:  0.035448007\n",
      "Epoch:  24 | Step:  0 | train loss:  0.031796362\n",
      "Epoch:  25 | Step:  0 | train loss:  0.028500568\n",
      "Epoch:  26 | Step:  0 | train loss:  0.025558619\n",
      "Epoch:  27 | Step:  0 | train loss:  0.02296449\n",
      "Epoch:  28 | Step:  0 | train loss:  0.020708421\n",
      "Epoch:  29 | Step:  0 | train loss:  0.01877646\n",
      "Epoch:  30 | Step:  0 | train loss:  0.0171506\n",
      "Epoch:  31 | Step:  0 | train loss:  0.015809191\n",
      "Epoch:  32 | Step:  0 | train loss:  0.014727627\n",
      "Epoch:  33 | Step:  0 | train loss:  0.013878791\n",
      "Epoch:  34 | Step:  0 | train loss:  0.0132339075\n",
      "Epoch:  35 | Step:  0 | train loss:  0.012763262\n",
      "Epoch:  36 | Step:  0 | train loss:  0.012437087\n",
      "Epoch:  37 | Step:  0 | train loss:  0.012226457\n",
      "Epoch:  38 | Step:  0 | train loss:  0.012104084\n",
      "Epoch:  39 | Step:  0 | train loss:  0.0120450165\n",
      "Epoch:  40 | Step:  0 | train loss:  0.012027205\n",
      "Epoch:  41 | Step:  0 | train loss:  0.0120318895\n",
      "Epoch:  42 | Step:  0 | train loss:  0.012043815\n",
      "Epoch:  43 | Step:  0 | train loss:  0.012051261\n",
      "Epoch:  44 | Step:  0 | train loss:  0.012045907\n",
      "Epoch:  45 | Step:  0 | train loss:  0.012022559\n",
      "Epoch:  46 | Step:  0 | train loss:  0.011978769\n",
      "Epoch:  47 | Step:  0 | train loss:  0.011914386\n",
      "Epoch:  48 | Step:  0 | train loss:  0.011831071\n",
      "Epoch:  49 | Step:  0 | train loss:  0.011731801\n",
      "Epoch:  50 | Step:  0 | train loss:  0.011620403\n",
      "Epoch:  51 | Step:  0 | train loss:  0.011501139\n",
      "Epoch:  52 | Step:  0 | train loss:  0.011378328\n",
      "Epoch:  53 | Step:  0 | train loss:  0.0112560615\n",
      "Epoch:  54 | Step:  0 | train loss:  0.011137971\n",
      "Epoch:  55 | Step:  0 | train loss:  0.011027075\n",
      "Epoch:  56 | Step:  0 | train loss:  0.010925694\n",
      "Epoch:  57 | Step:  0 | train loss:  0.010835412\n",
      "Epoch:  58 | Step:  0 | train loss:  0.010757104\n",
      "Epoch:  59 | Step:  0 | train loss:  0.010690985\n",
      "Epoch:  60 | Step:  0 | train loss:  0.010636705\n",
      "Epoch:  61 | Step:  0 | train loss:  0.01059345\n",
      "Epoch:  62 | Step:  0 | train loss:  0.010560056\n",
      "Epoch:  63 | Step:  0 | train loss:  0.01053513\n",
      "Epoch:  64 | Step:  0 | train loss:  0.0105171595\n",
      "Epoch:  65 | Step:  0 | train loss:  0.010504606\n",
      "Epoch:  66 | Step:  0 | train loss:  0.010496003\n",
      "Epoch:  67 | Step:  0 | train loss:  0.010490008\n",
      "Epoch:  68 | Step:  0 | train loss:  0.0104854675\n",
      "Epoch:  69 | Step:  0 | train loss:  0.010481435\n",
      "Epoch:  70 | Step:  0 | train loss:  0.010477194\n",
      "Epoch:  71 | Step:  0 | train loss:  0.010472249\n",
      "Epoch:  72 | Step:  0 | train loss:  0.010466318\n",
      "Epoch:  73 | Step:  0 | train loss:  0.010459298\n",
      "Epoch:  74 | Step:  0 | train loss:  0.010451247\n",
      "Epoch:  75 | Step:  0 | train loss:  0.010442335\n",
      "Epoch:  76 | Step:  0 | train loss:  0.010432816\n",
      "Epoch:  77 | Step:  0 | train loss:  0.010422988\n",
      "Epoch:  78 | Step:  0 | train loss:  0.010413162\n",
      "Epoch:  79 | Step:  0 | train loss:  0.010403638\n",
      "Epoch:  80 | Step:  0 | train loss:  0.010394674\n",
      "Epoch:  81 | Step:  0 | train loss:  0.010386482\n",
      "Epoch:  82 | Step:  0 | train loss:  0.01037921\n",
      "Epoch:  83 | Step:  0 | train loss:  0.010372944\n",
      "Epoch:  84 | Step:  0 | train loss:  0.010367709\n",
      "Epoch:  85 | Step:  0 | train loss:  0.010363474\n",
      "Epoch:  86 | Step:  0 | train loss:  0.010360161\n",
      "Epoch:  87 | Step:  0 | train loss:  0.010357659\n",
      "Epoch:  88 | Step:  0 | train loss:  0.010355836\n",
      "Epoch:  89 | Step:  0 | train loss:  0.010354548\n",
      "Epoch:  90 | Step:  0 | train loss:  0.010353653\n",
      "Epoch:  91 | Step:  0 | train loss:  0.010353017\n",
      "Epoch:  92 | Step:  0 | train loss:  0.010352525\n",
      "Epoch:  93 | Step:  0 | train loss:  0.010352083\n",
      "Epoch:  94 | Step:  0 | train loss:  0.01035162\n",
      "Epoch:  95 | Step:  0 | train loss:  0.010351091\n",
      "Epoch:  96 | Step:  0 | train loss:  0.0103504695\n",
      "Epoch:  97 | Step:  0 | train loss:  0.010349753\n",
      "Epoch:  98 | Step:  0 | train loss:  0.010348951\n",
      "Epoch:  99 | Step:  0 | train loss:  0.010348085\n",
      "Epoch:  100 | Step:  0 | train loss:  0.010347183\n",
      "Epoch:  101 | Step:  0 | train loss:  0.010346273\n",
      "Epoch:  102 | Step:  0 | train loss:  0.0103453845\n",
      "Epoch:  103 | Step:  0 | train loss:  0.010344542\n",
      "Epoch:  104 | Step:  0 | train loss:  0.010343762\n",
      "Epoch:  105 | Step:  0 | train loss:  0.010343056\n",
      "Epoch:  106 | Step:  0 | train loss:  0.010342428\n",
      "Epoch:  107 | Step:  0 | train loss:  0.010341874\n",
      "Epoch:  108 | Step:  0 | train loss:  0.010341383\n",
      "Epoch:  109 | Step:  0 | train loss:  0.01034094\n",
      "Epoch:  110 | Step:  0 | train loss:  0.010340528\n",
      "Epoch:  111 | Step:  0 | train loss:  0.010340125\n",
      "Epoch:  112 | Step:  0 | train loss:  0.010339716\n",
      "Epoch:  113 | Step:  0 | train loss:  0.010339283\n",
      "Epoch:  114 | Step:  0 | train loss:  0.010338813\n",
      "Epoch:  115 | Step:  0 | train loss:  0.0103383\n",
      "Epoch:  116 | Step:  0 | train loss:  0.01033774\n",
      "Epoch:  117 | Step:  0 | train loss:  0.01033714\n",
      "Epoch:  118 | Step:  0 | train loss:  0.010336512\n",
      "Epoch:  119 | Step:  0 | train loss:  0.010335871\n",
      "Epoch:  120 | Step:  0 | train loss:  0.01033524\n",
      "Epoch:  121 | Step:  0 | train loss:  0.010334639\n",
      "Epoch:  122 | Step:  0 | train loss:  0.010334087\n",
      "Epoch:  123 | Step:  0 | train loss:  0.010333592\n",
      "Epoch:  124 | Step:  0 | train loss:  0.010333159\n",
      "Epoch:  125 | Step:  0 | train loss:  0.010332785\n",
      "Epoch:  126 | Step:  0 | train loss:  0.010332464\n",
      "Epoch:  127 | Step:  0 | train loss:  0.01033219\n",
      "Epoch:  128 | Step:  0 | train loss:  0.010331958\n",
      "Epoch:  129 | Step:  0 | train loss:  0.010331759\n",
      "Epoch:  130 | Step:  0 | train loss:  0.010331588\n",
      "Epoch:  131 | Step:  0 | train loss:  0.010331438\n",
      "Epoch:  132 | Step:  0 | train loss:  0.010331307\n",
      "Epoch:  133 | Step:  0 | train loss:  0.010331189\n",
      "Epoch:  134 | Step:  0 | train loss:  0.010331081\n",
      "Epoch:  135 | Step:  0 | train loss:  0.0103309825\n",
      "Epoch:  136 | Step:  0 | train loss:  0.010330889\n",
      "Epoch:  137 | Step:  0 | train loss:  0.010330801\n",
      "Epoch:  138 | Step:  0 | train loss:  0.010330716\n",
      "Epoch:  139 | Step:  0 | train loss:  0.010330635\n",
      "Epoch:  140 | Step:  0 | train loss:  0.010330557\n",
      "Epoch:  141 | Step:  0 | train loss:  0.010330481\n",
      "Epoch:  142 | Step:  0 | train loss:  0.010330408\n",
      "Epoch:  143 | Step:  0 | train loss:  0.010330338\n",
      "Epoch:  144 | Step:  0 | train loss:  0.010330269\n",
      "Epoch:  145 | Step:  0 | train loss:  0.010330203\n",
      "Epoch:  146 | Step:  0 | train loss:  0.010330139\n",
      "Epoch:  147 | Step:  0 | train loss:  0.0103300745\n",
      "Epoch:  148 | Step:  0 | train loss:  0.010330011\n",
      "Epoch:  149 | Step:  0 | train loss:  0.010329948\n",
      "Epoch:  150 | Step:  0 | train loss:  0.010329883\n",
      "Epoch:  151 | Step:  0 | train loss:  0.0103298165\n",
      "Epoch:  152 | Step:  0 | train loss:  0.010329748\n",
      "Epoch:  153 | Step:  0 | train loss:  0.010329674\n",
      "Epoch:  154 | Step:  0 | train loss:  0.010329598\n",
      "Epoch:  155 | Step:  0 | train loss:  0.010329516\n",
      "Epoch:  156 | Step:  0 | train loss:  0.010329428\n",
      "Epoch:  157 | Step:  0 | train loss:  0.010329334\n",
      "Epoch:  158 | Step:  0 | train loss:  0.010329233\n",
      "Epoch:  159 | Step:  0 | train loss:  0.0103291245\n",
      "Epoch:  160 | Step:  0 | train loss:  0.010329008\n",
      "Epoch:  161 | Step:  0 | train loss:  0.010328884\n",
      "Epoch:  162 | Step:  0 | train loss:  0.010328751\n",
      "Epoch:  163 | Step:  0 | train loss:  0.01032861\n",
      "Epoch:  164 | Step:  0 | train loss:  0.010328461\n",
      "Epoch:  165 | Step:  0 | train loss:  0.010328304\n",
      "Epoch:  166 | Step:  0 | train loss:  0.010328139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.010327969\n",
      "Epoch:  168 | Step:  0 | train loss:  0.010327792\n",
      "Epoch:  169 | Step:  0 | train loss:  0.010327611\n",
      "Epoch:  170 | Step:  0 | train loss:  0.010327429\n",
      "Epoch:  171 | Step:  0 | train loss:  0.010327244\n",
      "Epoch:  172 | Step:  0 | train loss:  0.01032706\n",
      "Epoch:  173 | Step:  0 | train loss:  0.010326878\n",
      "Epoch:  174 | Step:  0 | train loss:  0.010326699\n",
      "Epoch:  175 | Step:  0 | train loss:  0.010326526\n",
      "Epoch:  176 | Step:  0 | train loss:  0.010326358\n",
      "Epoch:  177 | Step:  0 | train loss:  0.010326196\n",
      "Epoch:  178 | Step:  0 | train loss:  0.010326042\n",
      "Epoch:  179 | Step:  0 | train loss:  0.010325895\n",
      "Epoch:  180 | Step:  0 | train loss:  0.010325755\n",
      "Epoch:  181 | Step:  0 | train loss:  0.010325621\n",
      "Epoch:  182 | Step:  0 | train loss:  0.010325494\n",
      "Epoch:  183 | Step:  0 | train loss:  0.010325373\n",
      "Epoch:  184 | Step:  0 | train loss:  0.010325258\n",
      "Epoch:  185 | Step:  0 | train loss:  0.010325146\n",
      "Epoch:  186 | Step:  0 | train loss:  0.010325038\n",
      "Epoch:  187 | Step:  0 | train loss:  0.010324932\n",
      "Epoch:  188 | Step:  0 | train loss:  0.010324827\n",
      "Epoch:  189 | Step:  0 | train loss:  0.010324723\n",
      "Epoch:  190 | Step:  0 | train loss:  0.010324618\n",
      "Epoch:  191 | Step:  0 | train loss:  0.010324512\n",
      "Epoch:  192 | Step:  0 | train loss:  0.010324401\n",
      "Epoch:  193 | Step:  0 | train loss:  0.010324286\n",
      "Epoch:  194 | Step:  0 | train loss:  0.010324164\n",
      "Epoch:  195 | Step:  0 | train loss:  0.010324036\n",
      "Epoch:  196 | Step:  0 | train loss:  0.010323896\n",
      "Epoch:  197 | Step:  0 | train loss:  0.010323746\n",
      "Epoch:  198 | Step:  0 | train loss:  0.010323582\n",
      "Epoch:  199 | Step:  0 | train loss:  0.010323404\n",
      "Epoch:  200 | Step:  0 | train loss:  0.01032321\n",
      "Epoch:  201 | Step:  0 | train loss:  0.010322996\n",
      "Epoch:  202 | Step:  0 | train loss:  0.010322765\n",
      "Epoch:  203 | Step:  0 | train loss:  0.010322514\n",
      "Epoch:  204 | Step:  0 | train loss:  0.010322244\n",
      "Epoch:  205 | Step:  0 | train loss:  0.010321956\n",
      "Epoch:  206 | Step:  0 | train loss:  0.010321651\n",
      "Epoch:  207 | Step:  0 | train loss:  0.010321333\n",
      "Epoch:  208 | Step:  0 | train loss:  0.010321007\n",
      "Epoch:  209 | Step:  0 | train loss:  0.010320677\n",
      "Epoch:  210 | Step:  0 | train loss:  0.010320351\n",
      "Epoch:  211 | Step:  0 | train loss:  0.010320036\n",
      "Epoch:  212 | Step:  0 | train loss:  0.010319734\n",
      "Epoch:  213 | Step:  0 | train loss:  0.01031945\n",
      "Epoch:  214 | Step:  0 | train loss:  0.010319187\n",
      "Epoch:  215 | Step:  0 | train loss:  0.010318944\n",
      "Epoch:  216 | Step:  0 | train loss:  0.010318723\n",
      "Epoch:  217 | Step:  0 | train loss:  0.010318519\n",
      "Epoch:  218 | Step:  0 | train loss:  0.010318331\n",
      "Epoch:  219 | Step:  0 | train loss:  0.010318158\n",
      "Epoch:  220 | Step:  0 | train loss:  0.010317997\n",
      "Epoch:  221 | Step:  0 | train loss:  0.010317844\n",
      "Epoch:  222 | Step:  0 | train loss:  0.0103177\n",
      "Epoch:  223 | Step:  0 | train loss:  0.010317561\n",
      "Epoch:  224 | Step:  0 | train loss:  0.010317427\n",
      "Epoch:  225 | Step:  0 | train loss:  0.010317295\n",
      "Epoch:  226 | Step:  0 | train loss:  0.010317165\n",
      "Epoch:  227 | Step:  0 | train loss:  0.010317038\n",
      "Epoch:  228 | Step:  0 | train loss:  0.010316909\n",
      "Epoch:  229 | Step:  0 | train loss:  0.010316782\n",
      "Epoch:  230 | Step:  0 | train loss:  0.010316652\n",
      "Epoch:  231 | Step:  0 | train loss:  0.010316522\n",
      "Epoch:  232 | Step:  0 | train loss:  0.01031639\n",
      "Epoch:  233 | Step:  0 | train loss:  0.0103162555\n",
      "Epoch:  234 | Step:  0 | train loss:  0.010316118\n",
      "Epoch:  235 | Step:  0 | train loss:  0.010315976\n",
      "Epoch:  236 | Step:  0 | train loss:  0.010315832\n",
      "Epoch:  237 | Step:  0 | train loss:  0.010315683\n",
      "Epoch:  238 | Step:  0 | train loss:  0.010315528\n",
      "Epoch:  239 | Step:  0 | train loss:  0.010315369\n",
      "Epoch:  240 | Step:  0 | train loss:  0.010315204\n",
      "Epoch:  241 | Step:  0 | train loss:  0.010315034\n",
      "Epoch:  242 | Step:  0 | train loss:  0.010314856\n",
      "Epoch:  243 | Step:  0 | train loss:  0.010314671\n",
      "Epoch:  244 | Step:  0 | train loss:  0.01031448\n",
      "Epoch:  245 | Step:  0 | train loss:  0.010314282\n",
      "Epoch:  246 | Step:  0 | train loss:  0.010314075\n",
      "Epoch:  247 | Step:  0 | train loss:  0.010313862\n",
      "Epoch:  248 | Step:  0 | train loss:  0.01031364\n",
      "Epoch:  249 | Step:  0 | train loss:  0.01031341\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader): \n",
    "        # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8996e+00,  3.4054e+00,  3.2796e+00,  ...,  3.5001e+00,\n",
       "           3.5001e+00,  4.0002e+00],\n",
       "         [ 3.8913e+00,  3.3967e+00,  3.2726e+00,  ...,  3.4910e+00,\n",
       "           3.4909e+00,  3.9911e+00],\n",
       "         [ 3.8787e+00,  3.3833e+00,  3.2618e+00,  ...,  3.4772e+00,\n",
       "           3.4767e+00,  3.9773e+00],\n",
       "         ...,\n",
       "         [ 3.8995e+00,  3.4053e+00,  3.2795e+00,  ...,  3.5000e+00,\n",
       "           3.5000e+00,  4.0001e+00],\n",
       "         [ 3.8965e+00,  3.4021e+00,  3.2769e+00,  ...,  3.4967e+00,\n",
       "           3.4967e+00,  3.9967e+00],\n",
       "         [ 3.8995e+00,  3.4053e+00,  3.2795e+00,  ...,  3.5000e+00,\n",
       "           3.5000e+00,  4.0000e+00]]], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.15035919591335\n",
      "Autoencoder MAE: 1.3233262796224088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(get_mae(out, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
