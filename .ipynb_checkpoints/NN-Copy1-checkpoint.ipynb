{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.cuda()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.17166975\n",
      "Epoch:  1 | Step:  0 | train loss:  0.16247357\n",
      "Epoch:  2 | Step:  0 | train loss:  0.15365733\n",
      "Epoch:  3 | Step:  0 | train loss:  0.14473553\n",
      "Epoch:  4 | Step:  0 | train loss:  0.13569339\n",
      "Epoch:  5 | Step:  0 | train loss:  0.12650669\n",
      "Epoch:  6 | Step:  0 | train loss:  0.11719755\n",
      "Epoch:  7 | Step:  0 | train loss:  0.1077225\n",
      "Epoch:  8 | Step:  0 | train loss:  0.09806876\n",
      "Epoch:  9 | Step:  0 | train loss:  0.08826178\n",
      "Epoch:  10 | Step:  0 | train loss:  0.07852322\n",
      "Epoch:  11 | Step:  0 | train loss:  0.068975665\n",
      "Epoch:  12 | Step:  0 | train loss:  0.059837062\n",
      "Epoch:  13 | Step:  0 | train loss:  0.051265348\n",
      "Epoch:  14 | Step:  0 | train loss:  0.04337899\n",
      "Epoch:  15 | Step:  0 | train loss:  0.03629859\n",
      "Epoch:  16 | Step:  0 | train loss:  0.030129638\n",
      "Epoch:  17 | Step:  0 | train loss:  0.02493813\n",
      "Epoch:  18 | Step:  0 | train loss:  0.020748546\n",
      "Epoch:  19 | Step:  0 | train loss:  0.017542228\n",
      "Epoch:  20 | Step:  0 | train loss:  0.015256477\n",
      "Epoch:  21 | Step:  0 | train loss:  0.013790998\n",
      "Epoch:  22 | Step:  0 | train loss:  0.013013789\n",
      "Epoch:  23 | Step:  0 | train loss:  0.01277446\n",
      "Epoch:  24 | Step:  0 | train loss:  0.01291744\n",
      "Epoch:  25 | Step:  0 | train loss:  0.01329354\n",
      "Epoch:  26 | Step:  0 | train loss:  0.013769697\n",
      "Epoch:  27 | Step:  0 | train loss:  0.014237167\n",
      "Epoch:  28 | Step:  0 | train loss:  0.014615675\n",
      "Epoch:  29 | Step:  0 | train loss:  0.014853846\n",
      "Epoch:  30 | Step:  0 | train loss:  0.014926257\n",
      "Epoch:  31 | Step:  0 | train loss:  0.014830687\n",
      "Epoch:  32 | Step:  0 | train loss:  0.014581572\n",
      "Epoch:  33 | Step:  0 | train loss:  0.014205961\n",
      "Epoch:  34 | Step:  0 | train loss:  0.0137384115\n",
      "Epoch:  35 | Step:  0 | train loss:  0.013216201\n",
      "Epoch:  36 | Step:  0 | train loss:  0.012675779\n",
      "Epoch:  37 | Step:  0 | train loss:  0.012149909\n",
      "Epoch:  38 | Step:  0 | train loss:  0.01166563\n",
      "Epoch:  39 | Step:  0 | train loss:  0.011242781\n",
      "Epoch:  40 | Step:  0 | train loss:  0.010893627\n",
      "Epoch:  41 | Step:  0 | train loss:  0.010623116\n",
      "Epoch:  42 | Step:  0 | train loss:  0.010429587\n",
      "Epoch:  43 | Step:  0 | train loss:  0.010305895\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010240808\n",
      "Epoch:  45 | Step:  0 | train loss:  0.010220542\n",
      "Epoch:  46 | Step:  0 | train loss:  0.010230301\n",
      "Epoch:  47 | Step:  0 | train loss:  0.010255702\n",
      "Epoch:  48 | Step:  0 | train loss:  0.010283944\n",
      "Epoch:  49 | Step:  0 | train loss:  0.010304701\n",
      "Epoch:  50 | Step:  0 | train loss:  0.010310686\n",
      "Epoch:  51 | Step:  0 | train loss:  0.010297869\n",
      "Epoch:  52 | Step:  0 | train loss:  0.010265332\n",
      "Epoch:  53 | Step:  0 | train loss:  0.0102148475\n",
      "Epoch:  54 | Step:  0 | train loss:  0.01015025\n",
      "Epoch:  55 | Step:  0 | train loss:  0.010076677\n",
      "Epoch:  56 | Step:  0 | train loss:  0.00999979\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009925054\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009857136\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009799489\n",
      "Epoch:  60 | Step:  0 | train loss:  0.0097541325\n",
      "Epoch:  61 | Step:  0 | train loss:  0.009721626\n",
      "Epoch:  62 | Step:  0 | train loss:  0.009701212\n",
      "Epoch:  63 | Step:  0 | train loss:  0.009691093\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009688783\n",
      "Epoch:  65 | Step:  0 | train loss:  0.009691489\n",
      "Epoch:  66 | Step:  0 | train loss:  0.009696474\n",
      "Epoch:  67 | Step:  0 | train loss:  0.00970136\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009704328\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009704239\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009700641\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009693696\n",
      "Epoch:  72 | Step:  0 | train loss:  0.00968404\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009672611\n",
      "Epoch:  74 | Step:  0 | train loss:  0.009660471\n",
      "Epoch:  75 | Step:  0 | train loss:  0.0096486425\n",
      "Epoch:  76 | Step:  0 | train loss:  0.0096379835\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009629094\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009622284\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009617574\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009614736\n",
      "Epoch:  81 | Step:  0 | train loss:  0.00961338\n",
      "Epoch:  82 | Step:  0 | train loss:  0.00961303\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009613175\n",
      "Epoch:  84 | Step:  0 | train loss:  0.009613349\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009613198\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009612499\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009611166\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009609238\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009606843\n",
      "Epoch:  90 | Step:  0 | train loss:  0.0096041495\n",
      "Epoch:  91 | Step:  0 | train loss:  0.009601395\n",
      "Epoch:  92 | Step:  0 | train loss:  0.009598806\n",
      "Epoch:  93 | Step:  0 | train loss:  0.0095964875\n",
      "Epoch:  94 | Step:  0 | train loss:  0.0095945075\n",
      "Epoch:  95 | Step:  0 | train loss:  0.009592872\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009591535\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009590419\n",
      "Epoch:  98 | Step:  0 | train loss:  0.0095894225\n",
      "Epoch:  99 | Step:  0 | train loss:  0.00958842\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009587303\n",
      "Epoch:  101 | Step:  0 | train loss:  0.00958607\n",
      "Epoch:  102 | Step:  0 | train loss:  0.009584658\n",
      "Epoch:  103 | Step:  0 | train loss:  0.009583029\n",
      "Epoch:  104 | Step:  0 | train loss:  0.009581175\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009579078\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009577024\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009574922\n",
      "Epoch:  108 | Step:  0 | train loss:  0.009572779\n",
      "Epoch:  109 | Step:  0 | train loss:  0.009570508\n",
      "Epoch:  110 | Step:  0 | train loss:  0.009568129\n",
      "Epoch:  111 | Step:  0 | train loss:  0.009565808\n",
      "Epoch:  112 | Step:  0 | train loss:  0.009563334\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009560644\n",
      "Epoch:  114 | Step:  0 | train loss:  0.009557972\n",
      "Epoch:  115 | Step:  0 | train loss:  0.009555196\n",
      "Epoch:  116 | Step:  0 | train loss:  0.009552384\n",
      "Epoch:  117 | Step:  0 | train loss:  0.009549388\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009546141\n",
      "Epoch:  119 | Step:  0 | train loss:  0.009542472\n",
      "Epoch:  120 | Step:  0 | train loss:  0.009538193\n",
      "Epoch:  121 | Step:  0 | train loss:  0.009534375\n",
      "Epoch:  122 | Step:  0 | train loss:  0.009530083\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009524659\n",
      "Epoch:  124 | Step:  0 | train loss:  0.009519102\n",
      "Epoch:  125 | Step:  0 | train loss:  0.009513052\n",
      "Epoch:  126 | Step:  0 | train loss:  0.009506139\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009499025\n",
      "Epoch:  128 | Step:  0 | train loss:  0.0094908895\n",
      "Epoch:  129 | Step:  0 | train loss:  0.0094825355\n",
      "Epoch:  130 | Step:  0 | train loss:  0.009471426\n",
      "Epoch:  131 | Step:  0 | train loss:  0.009462253\n",
      "Epoch:  132 | Step:  0 | train loss:  0.009452674\n",
      "Epoch:  133 | Step:  0 | train loss:  0.009441828\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009430455\n",
      "Epoch:  135 | Step:  0 | train loss:  0.009416588\n",
      "Epoch:  136 | Step:  0 | train loss:  0.009400112\n",
      "Epoch:  137 | Step:  0 | train loss:  0.009390389\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009368092\n",
      "Epoch:  139 | Step:  0 | train loss:  0.009350216\n",
      "Epoch:  140 | Step:  0 | train loss:  0.009333902\n",
      "Epoch:  141 | Step:  0 | train loss:  0.009317461\n",
      "Epoch:  142 | Step:  0 | train loss:  0.0092992885\n",
      "Epoch:  143 | Step:  0 | train loss:  0.009281872\n",
      "Epoch:  144 | Step:  0 | train loss:  0.009266447\n",
      "Epoch:  145 | Step:  0 | train loss:  0.009250339\n",
      "Epoch:  146 | Step:  0 | train loss:  0.00922935\n",
      "Epoch:  147 | Step:  0 | train loss:  0.009212411\n",
      "Epoch:  148 | Step:  0 | train loss:  0.009195187\n",
      "Epoch:  149 | Step:  0 | train loss:  0.009174996\n",
      "Epoch:  150 | Step:  0 | train loss:  0.00914417\n",
      "Epoch:  151 | Step:  0 | train loss:  0.009113851\n",
      "Epoch:  152 | Step:  0 | train loss:  0.009092086\n",
      "Epoch:  153 | Step:  0 | train loss:  0.0090671815\n",
      "Epoch:  154 | Step:  0 | train loss:  0.009035406\n",
      "Epoch:  155 | Step:  0 | train loss:  0.009010324\n",
      "Epoch:  156 | Step:  0 | train loss:  0.0089793485\n",
      "Epoch:  157 | Step:  0 | train loss:  0.008957938\n",
      "Epoch:  158 | Step:  0 | train loss:  0.008928319\n",
      "Epoch:  159 | Step:  0 | train loss:  0.008901702\n",
      "Epoch:  160 | Step:  0 | train loss:  0.0088694515\n",
      "Epoch:  161 | Step:  0 | train loss:  0.008845355\n",
      "Epoch:  162 | Step:  0 | train loss:  0.008822031\n",
      "Epoch:  163 | Step:  0 | train loss:  0.008800079\n",
      "Epoch:  164 | Step:  0 | train loss:  0.008779463\n",
      "Epoch:  165 | Step:  0 | train loss:  0.008760427\n",
      "Epoch:  166 | Step:  0 | train loss:  0.008742422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.008725546\n",
      "Epoch:  168 | Step:  0 | train loss:  0.008712235\n",
      "Epoch:  169 | Step:  0 | train loss:  0.008687944\n",
      "Epoch:  170 | Step:  0 | train loss:  0.008662749\n",
      "Epoch:  171 | Step:  0 | train loss:  0.008646383\n",
      "Epoch:  172 | Step:  0 | train loss:  0.00863038\n",
      "Epoch:  173 | Step:  0 | train loss:  0.008615358\n",
      "Epoch:  174 | Step:  0 | train loss:  0.008607693\n",
      "Epoch:  175 | Step:  0 | train loss:  0.008584587\n",
      "Epoch:  176 | Step:  0 | train loss:  0.008569182\n",
      "Epoch:  177 | Step:  0 | train loss:  0.008554611\n",
      "Epoch:  178 | Step:  0 | train loss:  0.008543596\n",
      "Epoch:  179 | Step:  0 | train loss:  0.008531639\n",
      "Epoch:  180 | Step:  0 | train loss:  0.008513199\n",
      "Epoch:  181 | Step:  0 | train loss:  0.008502852\n",
      "Epoch:  182 | Step:  0 | train loss:  0.008489466\n",
      "Epoch:  183 | Step:  0 | train loss:  0.008478098\n",
      "Epoch:  184 | Step:  0 | train loss:  0.008505488\n",
      "Epoch:  185 | Step:  0 | train loss:  0.008480638\n",
      "Epoch:  186 | Step:  0 | train loss:  0.008566011\n",
      "Epoch:  187 | Step:  0 | train loss:  0.008460856\n",
      "Epoch:  188 | Step:  0 | train loss:  0.008431855\n",
      "Epoch:  189 | Step:  0 | train loss:  0.008415752\n",
      "Epoch:  190 | Step:  0 | train loss:  0.008409731\n",
      "Epoch:  191 | Step:  0 | train loss:  0.0084146\n",
      "Epoch:  192 | Step:  0 | train loss:  0.008402783\n",
      "Epoch:  193 | Step:  0 | train loss:  0.00839194\n",
      "Epoch:  194 | Step:  0 | train loss:  0.008390978\n",
      "Epoch:  195 | Step:  0 | train loss:  0.008377027\n",
      "Epoch:  196 | Step:  0 | train loss:  0.008363729\n",
      "Epoch:  197 | Step:  0 | train loss:  0.008354356\n",
      "Epoch:  198 | Step:  0 | train loss:  0.008347849\n",
      "Epoch:  199 | Step:  0 | train loss:  0.008331275\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(loader): \n",
    "        b_x = x.cuda()    # Tensor on GPU\n",
    "        b_y = y.cuda()    # Tensor on GPU\n",
    "        # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.1846, 3.7818, 3.4564,  ..., 3.4973, 3.4957, 4.4164],\n",
       "         [3.4967, 3.1559, 2.9297,  ..., 3.0665, 2.9483, 3.8570],\n",
       "         [4.1311, 3.7362, 3.4150,  ..., 3.4607, 3.4500, 4.3657],\n",
       "         ...,\n",
       "         [4.1508, 3.7510, 3.4317,  ..., 3.4763, 3.4686, 4.3879],\n",
       "         [4.1817, 3.7792, 3.4543,  ..., 3.4955, 3.4933, 4.4138],\n",
       "         [4.1529, 3.7528, 3.4332,  ..., 3.4776, 3.4702, 4.3897]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.2222300946698292\n",
      "Autoencoder MAE: 1.4938464043166197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(get_mae(out, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
