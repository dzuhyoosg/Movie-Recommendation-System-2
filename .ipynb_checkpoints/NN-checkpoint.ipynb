{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01 # learning rate set to 0.01\n",
    "BATCH_SIZE = 64 # batch size set  to 64\n",
    "EPOCH = 250 # run for 250 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "#75:25 train:test\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    # load ratings into 2d numpy array\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network's structure\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 50)\n",
    "        self.fc4 = nn.Linear(50, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.16952997\n",
      "Epoch:  1 | Step:  0 | train loss:  0.14799552\n",
      "Epoch:  2 | Step:  0 | train loss:  0.12715906\n",
      "Epoch:  3 | Step:  0 | train loss:  0.10669372\n",
      "Epoch:  4 | Step:  0 | train loss:  0.08686035\n",
      "Epoch:  5 | Step:  0 | train loss:  0.068255045\n",
      "Epoch:  6 | Step:  0 | train loss:  0.05148868\n",
      "Epoch:  7 | Step:  0 | train loss:  0.03726017\n",
      "Epoch:  8 | Step:  0 | train loss:  0.026145343\n",
      "Epoch:  9 | Step:  0 | train loss:  0.01843946\n",
      "Epoch:  10 | Step:  0 | train loss:  0.0141458465\n",
      "Epoch:  11 | Step:  0 | train loss:  0.012979386\n",
      "Epoch:  12 | Step:  0 | train loss:  0.01425484\n",
      "Epoch:  13 | Step:  0 | train loss:  0.016896825\n",
      "Epoch:  14 | Step:  0 | train loss:  0.019705849\n",
      "Epoch:  15 | Step:  0 | train loss:  0.021741267\n",
      "Epoch:  16 | Step:  0 | train loss:  0.022530701\n",
      "Epoch:  17 | Step:  0 | train loss:  0.022037413\n",
      "Epoch:  18 | Step:  0 | train loss:  0.02052057\n",
      "Epoch:  19 | Step:  0 | train loss:  0.018387424\n",
      "Epoch:  20 | Step:  0 | train loss:  0.016073074\n",
      "Epoch:  21 | Step:  0 | train loss:  0.013955019\n",
      "Epoch:  22 | Step:  0 | train loss:  0.012301626\n",
      "Epoch:  23 | Step:  0 | train loss:  0.011250341\n",
      "Epoch:  24 | Step:  0 | train loss:  0.010810612\n",
      "Epoch:  25 | Step:  0 | train loss:  0.010886763\n",
      "Epoch:  26 | Step:  0 | train loss:  0.011314096\n",
      "Epoch:  27 | Step:  0 | train loss:  0.01190046\n",
      "Epoch:  28 | Step:  0 | train loss:  0.012465165\n",
      "Epoch:  29 | Step:  0 | train loss:  0.012868263\n",
      "Epoch:  30 | Step:  0 | train loss:  0.013026408\n",
      "Epoch:  31 | Step:  0 | train loss:  0.012915546\n",
      "Epoch:  32 | Step:  0 | train loss:  0.012563616\n",
      "Epoch:  33 | Step:  0 | train loss:  0.012037243\n",
      "Epoch:  34 | Step:  0 | train loss:  0.011425622\n",
      "Epoch:  35 | Step:  0 | train loss:  0.010823686\n",
      "Epoch:  36 | Step:  0 | train loss:  0.010316032\n",
      "Epoch:  37 | Step:  0 | train loss:  0.009963143\n",
      "Epoch:  38 | Step:  0 | train loss:  0.009791829\n",
      "Epoch:  39 | Step:  0 | train loss:  0.009791929\n",
      "Epoch:  40 | Step:  0 | train loss:  0.009920649\n",
      "Epoch:  41 | Step:  0 | train loss:  0.010114207\n",
      "Epoch:  42 | Step:  0 | train loss:  0.010304141\n",
      "Epoch:  43 | Step:  0 | train loss:  0.01043385\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010470651\n",
      "Epoch:  45 | Step:  0 | train loss:  0.010410303\n",
      "Epoch:  46 | Step:  0 | train loss:  0.010273585\n",
      "Epoch:  47 | Step:  0 | train loss:  0.010097036\n",
      "Epoch:  48 | Step:  0 | train loss:  0.009921265\n",
      "Epoch:  49 | Step:  0 | train loss:  0.009780278\n",
      "Epoch:  50 | Step:  0 | train loss:  0.009694338\n",
      "Epoch:  51 | Step:  0 | train loss:  0.009667457\n",
      "Epoch:  52 | Step:  0 | train loss:  0.009689285\n",
      "Epoch:  53 | Step:  0 | train loss:  0.009740118\n",
      "Epoch:  54 | Step:  0 | train loss:  0.009797245\n",
      "Epoch:  55 | Step:  0 | train loss:  0.009840875\n",
      "Epoch:  56 | Step:  0 | train loss:  0.00985834\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009845905\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009808189\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009755716\n",
      "Epoch:  60 | Step:  0 | train loss:  0.00970141\n",
      "Epoch:  61 | Step:  0 | train loss:  0.00965698\n",
      "Epoch:  62 | Step:  0 | train loss:  0.009630057\n",
      "Epoch:  63 | Step:  0 | train loss:  0.0096227145\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009631624\n",
      "Epoch:  65 | Step:  0 | train loss:  0.0096496735\n",
      "Epoch:  66 | Step:  0 | train loss:  0.009668457\n",
      "Epoch:  67 | Step:  0 | train loss:  0.009680841\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009682828\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009674247\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009658203\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009639616\n",
      "Epoch:  72 | Step:  0 | train loss:  0.0096234465\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009613183\n",
      "Epoch:  74 | Step:  0 | train loss:  0.009610029\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009612907\n",
      "Epoch:  76 | Step:  0 | train loss:  0.009619161\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009625621\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009629655\n",
      "Epoch:  79 | Step:  0 | train loss:  0.009629882\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009626397\n",
      "Epoch:  81 | Step:  0 | train loss:  0.009620498\n",
      "Epoch:  82 | Step:  0 | train loss:  0.009614083\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009608947\n",
      "Epoch:  84 | Step:  0 | train loss:  0.0096062105\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009606055\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009607801\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009610264\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009612236\n",
      "Epoch:  89 | Step:  0 | train loss:  0.009612899\n",
      "Epoch:  90 | Step:  0 | train loss:  0.009612049\n",
      "Epoch:  91 | Step:  0 | train loss:  0.0096100615\n",
      "Epoch:  92 | Step:  0 | train loss:  0.00960767\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009605641\n",
      "Epoch:  94 | Step:  0 | train loss:  0.009604494\n",
      "Epoch:  95 | Step:  0 | train loss:  0.009604359\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009604985\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009605901\n",
      "Epoch:  98 | Step:  0 | train loss:  0.0096066175\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009606812\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009606417\n",
      "Epoch:  101 | Step:  0 | train loss:  0.009605603\n",
      "Epoch:  102 | Step:  0 | train loss:  0.009604678\n",
      "Epoch:  103 | Step:  0 | train loss:  0.009603943\n",
      "Epoch:  104 | Step:  0 | train loss:  0.009603582\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009603613\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009603897\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009604223\n",
      "Epoch:  108 | Step:  0 | train loss:  0.009604399\n",
      "Epoch:  109 | Step:  0 | train loss:  0.009604327\n",
      "Epoch:  110 | Step:  0 | train loss:  0.009604029\n",
      "Epoch:  111 | Step:  0 | train loss:  0.009603618\n",
      "Epoch:  112 | Step:  0 | train loss:  0.009603241\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009603015\n",
      "Epoch:  114 | Step:  0 | train loss:  0.009602967\n",
      "Epoch:  115 | Step:  0 | train loss:  0.009603045\n",
      "Epoch:  116 | Step:  0 | train loss:  0.009603158\n",
      "Epoch:  117 | Step:  0 | train loss:  0.009603207\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009603131\n",
      "Epoch:  119 | Step:  0 | train loss:  0.0096029155\n",
      "Epoch:  120 | Step:  0 | train loss:  0.0096026\n",
      "Epoch:  121 | Step:  0 | train loss:  0.009602256\n",
      "Epoch:  122 | Step:  0 | train loss:  0.0096019525\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009601705\n",
      "Epoch:  124 | Step:  0 | train loss:  0.009601466\n",
      "Epoch:  125 | Step:  0 | train loss:  0.009601194\n",
      "Epoch:  126 | Step:  0 | train loss:  0.009600884\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009600565\n",
      "Epoch:  128 | Step:  0 | train loss:  0.009600293\n",
      "Epoch:  129 | Step:  0 | train loss:  0.009600093\n",
      "Epoch:  130 | Step:  0 | train loss:  0.009599947\n",
      "Epoch:  131 | Step:  0 | train loss:  0.009599845\n",
      "Epoch:  132 | Step:  0 | train loss:  0.009599775\n",
      "Epoch:  133 | Step:  0 | train loss:  0.009599718\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009599648\n",
      "Epoch:  135 | Step:  0 | train loss:  0.009599537\n",
      "Epoch:  136 | Step:  0 | train loss:  0.009599363\n",
      "Epoch:  137 | Step:  0 | train loss:  0.009599149\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009598956\n",
      "Epoch:  139 | Step:  0 | train loss:  0.009598811\n",
      "Epoch:  140 | Step:  0 | train loss:  0.0095987\n",
      "Epoch:  141 | Step:  0 | train loss:  0.009598608\n",
      "Epoch:  142 | Step:  0 | train loss:  0.009598519\n",
      "Epoch:  143 | Step:  0 | train loss:  0.009598425\n",
      "Epoch:  144 | Step:  0 | train loss:  0.0095983185\n",
      "Epoch:  145 | Step:  0 | train loss:  0.0095982\n",
      "Epoch:  146 | Step:  0 | train loss:  0.009598076\n",
      "Epoch:  147 | Step:  0 | train loss:  0.009597952\n",
      "Epoch:  148 | Step:  0 | train loss:  0.009597829\n",
      "Epoch:  149 | Step:  0 | train loss:  0.0095977085\n",
      "Epoch:  150 | Step:  0 | train loss:  0.009597588\n",
      "Epoch:  151 | Step:  0 | train loss:  0.009597464\n",
      "Epoch:  152 | Step:  0 | train loss:  0.009597334\n",
      "Epoch:  153 | Step:  0 | train loss:  0.009597198\n",
      "Epoch:  154 | Step:  0 | train loss:  0.0095970575\n",
      "Epoch:  155 | Step:  0 | train loss:  0.009596914\n",
      "Epoch:  156 | Step:  0 | train loss:  0.009596772\n",
      "Epoch:  157 | Step:  0 | train loss:  0.009596629\n",
      "Epoch:  158 | Step:  0 | train loss:  0.009596485\n",
      "Epoch:  159 | Step:  0 | train loss:  0.009596337\n",
      "Epoch:  160 | Step:  0 | train loss:  0.009596185\n",
      "Epoch:  161 | Step:  0 | train loss:  0.009596029\n",
      "Epoch:  162 | Step:  0 | train loss:  0.009595869\n",
      "Epoch:  163 | Step:  0 | train loss:  0.009595707\n",
      "Epoch:  164 | Step:  0 | train loss:  0.009595544\n",
      "Epoch:  165 | Step:  0 | train loss:  0.00959538\n",
      "Epoch:  166 | Step:  0 | train loss:  0.009595213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.009595044\n",
      "Epoch:  168 | Step:  0 | train loss:  0.009594871\n",
      "Epoch:  169 | Step:  0 | train loss:  0.009594696\n",
      "Epoch:  170 | Step:  0 | train loss:  0.009594518\n",
      "Epoch:  171 | Step:  0 | train loss:  0.009594338\n",
      "Epoch:  172 | Step:  0 | train loss:  0.009594157\n",
      "Epoch:  173 | Step:  0 | train loss:  0.009593975\n",
      "Epoch:  174 | Step:  0 | train loss:  0.00959379\n",
      "Epoch:  175 | Step:  0 | train loss:  0.009593604\n",
      "Epoch:  176 | Step:  0 | train loss:  0.009593415\n",
      "Epoch:  177 | Step:  0 | train loss:  0.009593225\n",
      "Epoch:  178 | Step:  0 | train loss:  0.009593034\n",
      "Epoch:  179 | Step:  0 | train loss:  0.009592841\n",
      "Epoch:  180 | Step:  0 | train loss:  0.009592649\n",
      "Epoch:  181 | Step:  0 | train loss:  0.009592455\n",
      "Epoch:  182 | Step:  0 | train loss:  0.009592259\n",
      "Epoch:  183 | Step:  0 | train loss:  0.009592064\n",
      "Epoch:  184 | Step:  0 | train loss:  0.009591867\n",
      "Epoch:  185 | Step:  0 | train loss:  0.009591671\n",
      "Epoch:  186 | Step:  0 | train loss:  0.009591473\n",
      "Epoch:  187 | Step:  0 | train loss:  0.009591276\n",
      "Epoch:  188 | Step:  0 | train loss:  0.009591079\n",
      "Epoch:  189 | Step:  0 | train loss:  0.009590883\n",
      "Epoch:  190 | Step:  0 | train loss:  0.009590686\n",
      "Epoch:  191 | Step:  0 | train loss:  0.00959049\n",
      "Epoch:  192 | Step:  0 | train loss:  0.009590295\n",
      "Epoch:  193 | Step:  0 | train loss:  0.0095901005\n",
      "Epoch:  194 | Step:  0 | train loss:  0.009589907\n",
      "Epoch:  195 | Step:  0 | train loss:  0.009589714\n",
      "Epoch:  196 | Step:  0 | train loss:  0.009589522\n",
      "Epoch:  197 | Step:  0 | train loss:  0.009589332\n",
      "Epoch:  198 | Step:  0 | train loss:  0.009589144\n",
      "Epoch:  199 | Step:  0 | train loss:  0.009588957\n",
      "Epoch:  200 | Step:  0 | train loss:  0.009588771\n",
      "Epoch:  201 | Step:  0 | train loss:  0.009588587\n",
      "Epoch:  202 | Step:  0 | train loss:  0.0095884055\n",
      "Epoch:  203 | Step:  0 | train loss:  0.009588225\n",
      "Epoch:  204 | Step:  0 | train loss:  0.009588047\n",
      "Epoch:  205 | Step:  0 | train loss:  0.009587871\n",
      "Epoch:  206 | Step:  0 | train loss:  0.009587698\n",
      "Epoch:  207 | Step:  0 | train loss:  0.009587525\n",
      "Epoch:  208 | Step:  0 | train loss:  0.009587357\n",
      "Epoch:  209 | Step:  0 | train loss:  0.00958719\n",
      "Epoch:  210 | Step:  0 | train loss:  0.009587025\n",
      "Epoch:  211 | Step:  0 | train loss:  0.009586863\n",
      "Epoch:  212 | Step:  0 | train loss:  0.009586704\n",
      "Epoch:  213 | Step:  0 | train loss:  0.0095865475\n",
      "Epoch:  214 | Step:  0 | train loss:  0.009586393\n",
      "Epoch:  215 | Step:  0 | train loss:  0.009586241\n",
      "Epoch:  216 | Step:  0 | train loss:  0.009586092\n",
      "Epoch:  217 | Step:  0 | train loss:  0.009585945\n",
      "Epoch:  218 | Step:  0 | train loss:  0.0095858015\n",
      "Epoch:  219 | Step:  0 | train loss:  0.00958566\n",
      "Epoch:  220 | Step:  0 | train loss:  0.009585521\n",
      "Epoch:  221 | Step:  0 | train loss:  0.009585385\n",
      "Epoch:  222 | Step:  0 | train loss:  0.009585252\n",
      "Epoch:  223 | Step:  0 | train loss:  0.009585121\n",
      "Epoch:  224 | Step:  0 | train loss:  0.009584992\n",
      "Epoch:  225 | Step:  0 | train loss:  0.009584866\n",
      "Epoch:  226 | Step:  0 | train loss:  0.009584743\n",
      "Epoch:  227 | Step:  0 | train loss:  0.009584622\n",
      "Epoch:  228 | Step:  0 | train loss:  0.009584503\n",
      "Epoch:  229 | Step:  0 | train loss:  0.009584388\n",
      "Epoch:  230 | Step:  0 | train loss:  0.009584273\n",
      "Epoch:  231 | Step:  0 | train loss:  0.009584162\n",
      "Epoch:  232 | Step:  0 | train loss:  0.0095840525\n",
      "Epoch:  233 | Step:  0 | train loss:  0.009583945\n",
      "Epoch:  234 | Step:  0 | train loss:  0.009583841\n",
      "Epoch:  235 | Step:  0 | train loss:  0.009583739\n",
      "Epoch:  236 | Step:  0 | train loss:  0.009583637\n",
      "Epoch:  237 | Step:  0 | train loss:  0.009583538\n",
      "Epoch:  238 | Step:  0 | train loss:  0.0095834425\n",
      "Epoch:  239 | Step:  0 | train loss:  0.0095833475\n",
      "Epoch:  240 | Step:  0 | train loss:  0.009583254\n",
      "Epoch:  241 | Step:  0 | train loss:  0.009583163\n",
      "Epoch:  242 | Step:  0 | train loss:  0.009583074\n",
      "Epoch:  243 | Step:  0 | train loss:  0.009582986\n",
      "Epoch:  244 | Step:  0 | train loss:  0.0095828995\n",
      "Epoch:  245 | Step:  0 | train loss:  0.009582815\n",
      "Epoch:  246 | Step:  0 | train loss:  0.009582732\n",
      "Epoch:  247 | Step:  0 | train loss:  0.009582651\n",
      "Epoch:  248 | Step:  0 | train loss:  0.00958257\n",
      "Epoch:  249 | Step:  0 | train loss:  0.009582492\n",
      "Runtime: 417.22192335128784seconds\n"
     ]
    }
   ],
   "source": [
    "# training process\n",
    "loss_his = []\n",
    "st = time.time()\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader): \n",
    "        # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())\n",
    "\n",
    "print('Runtime: ' + str(time.time()-st) + 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training loss visualization\n",
    "plt.plot(range(EPOCH), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.9682, 3.4754, 3.1795,  ..., 3.5001, 3.5000, 4.0001],\n",
       "         [3.9682, 3.4753, 3.1795,  ..., 3.5000, 3.5000, 4.0000],\n",
       "         [3.9665, 3.4739, 3.1785,  ..., 3.4983, 3.4985, 3.9982],\n",
       "         ...,\n",
       "         [3.9682, 3.4754, 3.1795,  ..., 3.5001, 3.5000, 4.0001],\n",
       "         [3.9682, 3.4753, 3.1795,  ..., 3.5000, 3.5000, 4.0000],\n",
       "         [3.9682, 3.4754, 3.1795,  ..., 3.5001, 3.5000, 4.0001]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.detach().numpy()[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder RMSE: 1.1848960143229332\n",
      "Autoencoder MAE: 1.403978564758373\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "def get_mae(pred, actual):\n",
    "    # Ignore zero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "test = user_item_matrix(test_set)\n",
    "print('Autoencoder RMSE: ' + str(math.sqrt(get_mse(out, test))))\n",
    "print('Autoencoder MAE: ' + str(get_mae(out, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
