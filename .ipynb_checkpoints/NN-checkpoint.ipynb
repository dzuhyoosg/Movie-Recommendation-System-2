{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./ml-latest-small/ratings.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75627, 25209)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection as ms\n",
    "train_set, test_set = ms.train_test_split(df, test_size=0.25)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array(train_set, dtype = 'int')\n",
    "testing_set = np.array(test_set, dtype = 'int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 9724)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users = df.userId.unique().shape[0]\n",
    "num_items = df.movieId.unique().shape[0]\n",
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_movieId = df.movieId.unique().tolist()\n",
    "movie_movieId.sort()\n",
    "d = dict()\n",
    "for i in range(0, len(movie_movieId)):\n",
    "    d[movie_movieId[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_item_matrix(data):\n",
    "    ratings = np.zeros((num_users, num_items))\n",
    "    for row in data.itertuples():\n",
    "        ratings[row[1]-1, d[row[2]]] = row[3]\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = user_item_matrix(df)\n",
    "train = user_item_matrix(train_set)\n",
    "test = user_item_matrix(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 610, 9724])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.FloatTensor(train)\n",
    "test = torch.FloatTensor(test)\n",
    "input = Variable(train).unsqueeze(0)\n",
    "target = input\n",
    "torch_dataset = Data.TensorDataset(input, target)\n",
    "loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_items, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, num_items)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.activation_t = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation_t(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "loss_func = nn.MSELoss()\n",
    "opt = torch.optim.Adam(net.parameters(), lr=LR, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | train loss:  0.1714545\n",
      "Epoch:  1 | Step:  0 | train loss:  0.16235329\n",
      "Epoch:  2 | Step:  0 | train loss:  0.15320891\n",
      "Epoch:  3 | Step:  0 | train loss:  0.14375944\n",
      "Epoch:  4 | Step:  0 | train loss:  0.13422026\n",
      "Epoch:  5 | Step:  0 | train loss:  0.124658205\n",
      "Epoch:  6 | Step:  0 | train loss:  0.11509145\n",
      "Epoch:  7 | Step:  0 | train loss:  0.10542398\n",
      "Epoch:  8 | Step:  0 | train loss:  0.09562868\n",
      "Epoch:  9 | Step:  0 | train loss:  0.0855278\n",
      "Epoch:  10 | Step:  0 | train loss:  0.07522773\n",
      "Epoch:  11 | Step:  0 | train loss:  0.064958654\n",
      "Epoch:  12 | Step:  0 | train loss:  0.05480925\n",
      "Epoch:  13 | Step:  0 | train loss:  0.045225646\n",
      "Epoch:  14 | Step:  0 | train loss:  0.036797103\n",
      "Epoch:  15 | Step:  0 | train loss:  0.029668422\n",
      "Epoch:  16 | Step:  0 | train loss:  0.023857968\n",
      "Epoch:  17 | Step:  0 | train loss:  0.019487152\n",
      "Epoch:  18 | Step:  0 | train loss:  0.016486151\n",
      "Epoch:  19 | Step:  0 | train loss:  0.014673841\n",
      "Epoch:  20 | Step:  0 | train loss:  0.013821855\n",
      "Epoch:  21 | Step:  0 | train loss:  0.013670017\n",
      "Epoch:  22 | Step:  0 | train loss:  0.013967502\n",
      "Epoch:  23 | Step:  0 | train loss:  0.01448145\n",
      "Epoch:  24 | Step:  0 | train loss:  0.015036384\n",
      "Epoch:  25 | Step:  0 | train loss:  0.015511709\n",
      "Epoch:  26 | Step:  0 | train loss:  0.015830453\n",
      "Epoch:  27 | Step:  0 | train loss:  0.01595493\n",
      "Epoch:  28 | Step:  0 | train loss:  0.015878357\n",
      "Epoch:  29 | Step:  0 | train loss:  0.015616819\n",
      "Epoch:  30 | Step:  0 | train loss:  0.015202012\n",
      "Epoch:  31 | Step:  0 | train loss:  0.0146748135\n",
      "Epoch:  32 | Step:  0 | train loss:  0.014079675\n",
      "Epoch:  33 | Step:  0 | train loss:  0.013459831\n",
      "Epoch:  34 | Step:  0 | train loss:  0.012853562\n",
      "Epoch:  35 | Step:  0 | train loss:  0.012291641\n",
      "Epoch:  36 | Step:  0 | train loss:  0.011796015\n",
      "Epoch:  37 | Step:  0 | train loss:  0.0113796545\n",
      "Epoch:  38 | Step:  0 | train loss:  0.01104727\n",
      "Epoch:  39 | Step:  0 | train loss:  0.010796619\n",
      "Epoch:  40 | Step:  0 | train loss:  0.010620063\n",
      "Epoch:  41 | Step:  0 | train loss:  0.010506206\n",
      "Epoch:  42 | Step:  0 | train loss:  0.010441449\n",
      "Epoch:  43 | Step:  0 | train loss:  0.01041138\n",
      "Epoch:  44 | Step:  0 | train loss:  0.010401998\n",
      "Epoch:  45 | Step:  0 | train loss:  0.010400735\n",
      "Epoch:  46 | Step:  0 | train loss:  0.010397268\n",
      "Epoch:  47 | Step:  0 | train loss:  0.010384066\n",
      "Epoch:  48 | Step:  0 | train loss:  0.010356634\n",
      "Epoch:  49 | Step:  0 | train loss:  0.010313449\n",
      "Epoch:  50 | Step:  0 | train loss:  0.010255624\n",
      "Epoch:  51 | Step:  0 | train loss:  0.010186343\n",
      "Epoch:  52 | Step:  0 | train loss:  0.010110164\n",
      "Epoch:  53 | Step:  0 | train loss:  0.010032184\n",
      "Epoch:  54 | Step:  0 | train loss:  0.009957284\n",
      "Epoch:  55 | Step:  0 | train loss:  0.009889535\n",
      "Epoch:  56 | Step:  0 | train loss:  0.009831839\n",
      "Epoch:  57 | Step:  0 | train loss:  0.009785755\n",
      "Epoch:  58 | Step:  0 | train loss:  0.009751516\n",
      "Epoch:  59 | Step:  0 | train loss:  0.009728209\n",
      "Epoch:  60 | Step:  0 | train loss:  0.009714045\n",
      "Epoch:  61 | Step:  0 | train loss:  0.00970671\n",
      "Epoch:  62 | Step:  0 | train loss:  0.009703724\n",
      "Epoch:  63 | Step:  0 | train loss:  0.009702734\n",
      "Epoch:  64 | Step:  0 | train loss:  0.009701769\n",
      "Epoch:  65 | Step:  0 | train loss:  0.00969938\n",
      "Epoch:  66 | Step:  0 | train loss:  0.00969471\n",
      "Epoch:  67 | Step:  0 | train loss:  0.009687467\n",
      "Epoch:  68 | Step:  0 | train loss:  0.009677842\n",
      "Epoch:  69 | Step:  0 | train loss:  0.009666387\n",
      "Epoch:  70 | Step:  0 | train loss:  0.009653865\n",
      "Epoch:  71 | Step:  0 | train loss:  0.009641115\n",
      "Epoch:  72 | Step:  0 | train loss:  0.009628921\n",
      "Epoch:  73 | Step:  0 | train loss:  0.009617926\n",
      "Epoch:  74 | Step:  0 | train loss:  0.009608565\n",
      "Epoch:  75 | Step:  0 | train loss:  0.009601047\n",
      "Epoch:  76 | Step:  0 | train loss:  0.00959536\n",
      "Epoch:  77 | Step:  0 | train loss:  0.009591306\n",
      "Epoch:  78 | Step:  0 | train loss:  0.009588555\n",
      "Epoch:  79 | Step:  0 | train loss:  0.0095867105\n",
      "Epoch:  80 | Step:  0 | train loss:  0.009585358\n",
      "Epoch:  81 | Step:  0 | train loss:  0.009584127\n",
      "Epoch:  82 | Step:  0 | train loss:  0.0095827235\n",
      "Epoch:  83 | Step:  0 | train loss:  0.009580958\n",
      "Epoch:  84 | Step:  0 | train loss:  0.009578748\n",
      "Epoch:  85 | Step:  0 | train loss:  0.009576091\n",
      "Epoch:  86 | Step:  0 | train loss:  0.009573018\n",
      "Epoch:  87 | Step:  0 | train loss:  0.009569732\n",
      "Epoch:  88 | Step:  0 | train loss:  0.009566341\n",
      "Epoch:  89 | Step:  0 | train loss:  0.00956303\n",
      "Epoch:  90 | Step:  0 | train loss:  0.009559951\n",
      "Epoch:  91 | Step:  0 | train loss:  0.009557152\n",
      "Epoch:  92 | Step:  0 | train loss:  0.009554454\n",
      "Epoch:  93 | Step:  0 | train loss:  0.009551254\n",
      "Epoch:  94 | Step:  0 | train loss:  0.009548298\n",
      "Epoch:  95 | Step:  0 | train loss:  0.00954611\n",
      "Epoch:  96 | Step:  0 | train loss:  0.009543289\n",
      "Epoch:  97 | Step:  0 | train loss:  0.009540353\n",
      "Epoch:  98 | Step:  0 | train loss:  0.009537046\n",
      "Epoch:  99 | Step:  0 | train loss:  0.009533163\n",
      "Epoch:  100 | Step:  0 | train loss:  0.009529452\n",
      "Epoch:  101 | Step:  0 | train loss:  0.0095246425\n",
      "Epoch:  102 | Step:  0 | train loss:  0.009516432\n",
      "Epoch:  103 | Step:  0 | train loss:  0.009511219\n",
      "Epoch:  104 | Step:  0 | train loss:  0.009505726\n",
      "Epoch:  105 | Step:  0 | train loss:  0.009497626\n",
      "Epoch:  106 | Step:  0 | train loss:  0.009491636\n",
      "Epoch:  107 | Step:  0 | train loss:  0.009485395\n",
      "Epoch:  108 | Step:  0 | train loss:  0.009478945\n",
      "Epoch:  109 | Step:  0 | train loss:  0.009469087\n",
      "Epoch:  110 | Step:  0 | train loss:  0.00945795\n",
      "Epoch:  111 | Step:  0 | train loss:  0.009449503\n",
      "Epoch:  112 | Step:  0 | train loss:  0.009439548\n",
      "Epoch:  113 | Step:  0 | train loss:  0.009430762\n",
      "Epoch:  114 | Step:  0 | train loss:  0.00942401\n",
      "Epoch:  115 | Step:  0 | train loss:  0.009418363\n",
      "Epoch:  116 | Step:  0 | train loss:  0.00941272\n",
      "Epoch:  117 | Step:  0 | train loss:  0.0094069475\n",
      "Epoch:  118 | Step:  0 | train loss:  0.009401602\n",
      "Epoch:  119 | Step:  0 | train loss:  0.00939637\n",
      "Epoch:  120 | Step:  0 | train loss:  0.009390686\n",
      "Epoch:  121 | Step:  0 | train loss:  0.00938437\n",
      "Epoch:  122 | Step:  0 | train loss:  0.009376541\n",
      "Epoch:  123 | Step:  0 | train loss:  0.009369653\n",
      "Epoch:  124 | Step:  0 | train loss:  0.009362209\n",
      "Epoch:  125 | Step:  0 | train loss:  0.009352784\n",
      "Epoch:  126 | Step:  0 | train loss:  0.009342654\n",
      "Epoch:  127 | Step:  0 | train loss:  0.009335026\n",
      "Epoch:  128 | Step:  0 | train loss:  0.009326668\n",
      "Epoch:  129 | Step:  0 | train loss:  0.009317167\n",
      "Epoch:  130 | Step:  0 | train loss:  0.009304308\n",
      "Epoch:  131 | Step:  0 | train loss:  0.0092890775\n",
      "Epoch:  132 | Step:  0 | train loss:  0.009273448\n",
      "Epoch:  133 | Step:  0 | train loss:  0.009254853\n",
      "Epoch:  134 | Step:  0 | train loss:  0.009232807\n",
      "Epoch:  135 | Step:  0 | train loss:  0.009209895\n",
      "Epoch:  136 | Step:  0 | train loss:  0.009182048\n",
      "Epoch:  137 | Step:  0 | train loss:  0.009147831\n",
      "Epoch:  138 | Step:  0 | train loss:  0.009109181\n",
      "Epoch:  139 | Step:  0 | train loss:  0.009074277\n",
      "Epoch:  140 | Step:  0 | train loss:  0.009029298\n",
      "Epoch:  141 | Step:  0 | train loss:  0.008982484\n",
      "Epoch:  142 | Step:  0 | train loss:  0.00892608\n",
      "Epoch:  143 | Step:  0 | train loss:  0.008861465\n",
      "Epoch:  144 | Step:  0 | train loss:  0.008791878\n",
      "Epoch:  145 | Step:  0 | train loss:  0.008739178\n",
      "Epoch:  146 | Step:  0 | train loss:  0.008692436\n",
      "Epoch:  147 | Step:  0 | train loss:  0.00864833\n",
      "Epoch:  148 | Step:  0 | train loss:  0.0086141955\n",
      "Epoch:  149 | Step:  0 | train loss:  0.008591879\n",
      "Epoch:  150 | Step:  0 | train loss:  0.008550479\n",
      "Epoch:  151 | Step:  0 | train loss:  0.008544526\n",
      "Epoch:  152 | Step:  0 | train loss:  0.008552549\n",
      "Epoch:  153 | Step:  0 | train loss:  0.008507144\n",
      "Epoch:  154 | Step:  0 | train loss:  0.008507357\n",
      "Epoch:  155 | Step:  0 | train loss:  0.008505157\n",
      "Epoch:  156 | Step:  0 | train loss:  0.0084867515\n",
      "Epoch:  157 | Step:  0 | train loss:  0.008474836\n",
      "Epoch:  158 | Step:  0 | train loss:  0.008467384\n",
      "Epoch:  159 | Step:  0 | train loss:  0.008470476\n",
      "Epoch:  160 | Step:  0 | train loss:  0.008453426\n",
      "Epoch:  161 | Step:  0 | train loss:  0.008435971\n",
      "Epoch:  162 | Step:  0 | train loss:  0.008432152\n",
      "Epoch:  163 | Step:  0 | train loss:  0.008417294\n",
      "Epoch:  164 | Step:  0 | train loss:  0.008405465\n",
      "Epoch:  165 | Step:  0 | train loss:  0.008364763\n",
      "Epoch:  166 | Step:  0 | train loss:  0.008351958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  167 | Step:  0 | train loss:  0.00834526\n",
      "Epoch:  168 | Step:  0 | train loss:  0.008338408\n",
      "Epoch:  169 | Step:  0 | train loss:  0.008318281\n",
      "Epoch:  170 | Step:  0 | train loss:  0.008309253\n",
      "Epoch:  171 | Step:  0 | train loss:  0.008298546\n",
      "Epoch:  172 | Step:  0 | train loss:  0.008277877\n",
      "Epoch:  173 | Step:  0 | train loss:  0.008275408\n",
      "Epoch:  174 | Step:  0 | train loss:  0.008265298\n",
      "Epoch:  175 | Step:  0 | train loss:  0.008252696\n",
      "Epoch:  176 | Step:  0 | train loss:  0.008246388\n",
      "Epoch:  177 | Step:  0 | train loss:  0.008241717\n",
      "Epoch:  178 | Step:  0 | train loss:  0.008234271\n",
      "Epoch:  179 | Step:  0 | train loss:  0.008225619\n",
      "Epoch:  180 | Step:  0 | train loss:  0.008215844\n",
      "Epoch:  181 | Step:  0 | train loss:  0.008203404\n",
      "Epoch:  182 | Step:  0 | train loss:  0.008200728\n",
      "Epoch:  183 | Step:  0 | train loss:  0.008184218\n",
      "Epoch:  184 | Step:  0 | train loss:  0.008169106\n",
      "Epoch:  185 | Step:  0 | train loss:  0.008100799\n",
      "Epoch:  186 | Step:  0 | train loss:  0.008083685\n",
      "Epoch:  187 | Step:  0 | train loss:  0.008077351\n",
      "Epoch:  188 | Step:  0 | train loss:  0.008064553\n",
      "Epoch:  189 | Step:  0 | train loss:  0.008050481\n",
      "Epoch:  190 | Step:  0 | train loss:  0.0080311075\n",
      "Epoch:  191 | Step:  0 | train loss:  0.008006252\n",
      "Epoch:  192 | Step:  0 | train loss:  0.007992015\n",
      "Epoch:  193 | Step:  0 | train loss:  0.007971938\n",
      "Epoch:  194 | Step:  0 | train loss:  0.007957875\n",
      "Epoch:  195 | Step:  0 | train loss:  0.007951063\n",
      "Epoch:  196 | Step:  0 | train loss:  0.007936965\n",
      "Epoch:  197 | Step:  0 | train loss:  0.007929397\n",
      "Epoch:  198 | Step:  0 | train loss:  0.007917701\n",
      "Epoch:  199 | Step:  0 | train loss:  0.007907298\n"
     ]
    }
   ],
   "source": [
    "loss_his = []\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(loader):          # for each training step\n",
    "        output = net(b_x)  \n",
    "        output[target == 0] = 0\n",
    "        # get output for every net\n",
    "        loss = loss_func(output, b_y)  # compute loss for every net\n",
    "        opt.zero_grad()                # clear gradients for next train\n",
    "        loss.backward()                # backpropagation, compute gradients\n",
    "        opt.step()                     # apply gradients\n",
    "        loss_his.append(loss.data.numpy())     # loss recoder\n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| train loss: ', loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(200), loss_his)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.8163e+00,  3.3626e+00,  3.0566e+00,  ...,  2.9495e+00,\n",
       "           3.1083e+00,  3.8543e+00],\n",
       "         [ 4.1198e+00,  3.7621e+00,  3.3828e+00,  ...,  3.2095e+00,\n",
       "           3.3109e+00,  4.1500e+00],\n",
       "         [ 4.0444e+00,  3.6753e+00,  3.3184e+00,  ...,  3.1614e+00,\n",
       "           3.2451e+00,  4.0561e+00],\n",
       "         ...,\n",
       "         [ 3.7956e+00,  3.3361e+00,  3.0353e+00,  ...,  2.9321e+00,\n",
       "           3.0934e+00,  3.8338e+00],\n",
       "         [ 3.9480e+00,  3.5365e+00,  3.2000e+00,  ...,  3.0642e+00,\n",
       "           3.1980e+00,  3.9823e+00],\n",
       "         [ 3.7956e+00,  3.3361e+00,  3.0353e+00,  ...,  2.9320e+00,\n",
       "           3.0934e+00,  3.8338e+00]]], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_var = Variable(test).unsqueeze(0)\n",
    "out = net(test_var)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-624d0d57408e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-624d0d57408e>\u001b[0m in \u001b[0;36mget_mse\u001b[0;34m(pred, actual)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cu/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 239\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
      "\u001b[0;32m~/anaconda3/envs/cu/lib/python3.6/site-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cu/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/envs/cu/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cu/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred.flatten()\n",
    "    actual = actual.flatten()\n",
    "    return mean_squared_error(actual, pred)\n",
    "\n",
    "print(get_mse(out, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
